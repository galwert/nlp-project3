{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galwert/nlp-project3/blob/main/project3_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "369c1284",
      "metadata": {
        "collapsed": true,
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "369c1284",
        "outputId": "50c0895f-5150-4a55-cc1c-01be56adc081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 67.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.0/164.0 kB 19.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 20.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 14.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.6/232.6 kB 24.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 99.4 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "\n",
        "       Prints the result to stdout and returns the exit status.\n",
        "       Provides a printed warning on non-zero exit status unless `warn`\n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/project3.git .tmp\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0c95073",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d0c95073"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "e3c6f7d7",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "e3c6f7d7"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91dbe3a4",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "91dbe3a4"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20be92be",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "20be92be"
      },
      "source": [
        "# \t236299 - Introduction to Natural Language Processing\n",
        "## Project 3: Parsing – The CKY Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce60c947",
      "metadata": {
        "id": "ce60c947"
      },
      "source": [
        "Constituency parsing is the recovery of a labeled hierarchical structure, a _parse tree_ for a sentence of a natural language. It is a core intermediary task in natural-language processing, as the meanings of sentences are related to their structure.\n",
        "\n",
        "In this project, you will implement the CKY algorithm for parsing strings relative to context-free grammars (CFG). You will implement versions for both non-probabilistic context-free grammars (CFG) and probabilistic grammars (PCFG) and apply them to the parsing of ATIS queries.\n",
        "\n",
        "The project is structured into five parts:\n",
        "\n",
        "1. Finish a CFG for the ATIS dataset.\n",
        "2. Implement the CKY algorithm for _recognizing_ grammatical sentences, that is, determining whether a parse exists for a given sentence.\n",
        "3. Extend the CKY algorithm for _parsing_ sentences, that is, constructing the parse trees for a given sentence.\n",
        "4. Construct a probabilistic context-free grammar (PCFG) based on a CFG.\n",
        "5. Extend the CKY algorithm to PCFGs, allowing the construction of the most probable parse tree for a sentence according to a PCFG.\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d132d71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1d132d71",
        "outputId": "e31d08ed-3559-41ec-d77f-2e2edab1c223"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scripts//tree.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Download needed files and scripts\n",
        "import wget\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('scripts', exist_ok=True)\n",
        "# ATIS queries\n",
        "wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/train.nl\", out=\"data/\")\n",
        "# Corresponding parse trees\n",
        "wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/train.trees\", out=\"data/\")\n",
        "wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/test.trees\", out=\"data/\")\n",
        "\n",
        "# Code for comparing and evaluating parse trees\n",
        "wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/scripts/trees/evalb.py\", out=\"scripts/\")\n",
        "wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/scripts/trees/transform.py\", out=\"scripts/\")\n",
        "wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/scripts/trees/tree.py\", out=\"scripts/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a40322df",
      "metadata": {
        "id": "a40322df"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "import nltk\n",
        "\n",
        "import sys\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from nltk import treetransforms\n",
        "from nltk.grammar import ProbabilisticProduction, PCFG, Nonterminal\n",
        "from nltk.tree import Tree\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import functions for transforming augmented grammars\n",
        "sys.path.insert(1, './scripts')\n",
        "import transform as xform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "67a9a24a",
      "metadata": {
        "id": "67a9a24a"
      },
      "outputs": [],
      "source": [
        "## Debug flag used below for turning on and off some useful tracing\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f534761",
      "metadata": {
        "id": "6f534761"
      },
      "source": [
        "# A custom ATIS grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16392588",
      "metadata": {
        "id": "16392588"
      },
      "source": [
        "To parse, we need a grammar. In this project, you will use a hand-crafted grammar for a fragment of the ATIS dataset. The grammar is written in a \"semantic grammar\" style, in which the nonterminals tend to correspond to semantic classes of phrases, rather than syntactic classes. By using this style, we can more closely tune the grammar to the application, though we lose generality and transferability to other applications. The grammar will be used again in the next project segment for a question-answering application.\n",
        "\n",
        "We download the grammar to make it available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b5a525a4",
      "metadata": {
        "id": "b5a525a4"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./data/grammar_distrib3'):\n",
        "  wget.download(\"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/grammar_distrib3\", out=\"data/\")\n",
        "if os.path.exists('./data/grammar_distrib3') and (not os.path.exists('./data/grammar')):\n",
        "  shutil.copy('./data/grammar_distrib3', './data/grammar')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2b8c6b",
      "metadata": {
        "id": "8e2b8c6b"
      },
      "source": [
        "Take a look at the file `data/grammar_distrib3` that you've just downloaded. The grammar is written in a format that extends the NLTK format expected by `CFG.fromstring`. We've provided functions to make use of this format in the file `scripts/transform.py`. You should familiarize yourself with this format by checking out the documentation in that file.\n",
        "\n",
        "> We made a copy of this grammar for you as `data/grammar`. This is the file you'll be modifying in the next section. You can leave it alone for now.\n",
        "\n",
        "As described there, we can read the grammar in and convert it into NLTK's grammar format using the provided `xform.read_augmented_grammar` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "1acfc132",
      "metadata": {
        "id": "1acfc132"
      },
      "outputs": [],
      "source": [
        "atis_grammar_distrib, _ = xform.read_augmented_grammar(\"grammar_distrib3\", path=\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54641ed0",
      "metadata": {
        "id": "54641ed0"
      },
      "source": [
        "To verify that the ATIS grammar that we distributed is working, we can parse a sentence using a built-in NLTK parser. We'll use a tokenizer built with NLTK's tokenizing apparatus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "77f1e572",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77f1e572",
        "outputId": "1acd677d-7084-45a1-e3f9-49b36fc25758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are', 'there', 'any', 'first-class', 'flights', 'at', '11', 'pm', 'for', 'less', 'than', '$3.50', '?']\n"
          ]
        }
      ],
      "source": [
        "## Tokenizer\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer('\\d+|[\\w-]+|\\$[\\d\\.]+|\\S+')\n",
        "def tokenize(string):\n",
        "  return tokenizer.tokenize(string.lower())\n",
        "\n",
        "## Demonstrating the tokenizer\n",
        "## Note especially the handling of `\"11pm\"` and hyphenated words.\n",
        "print(tokenize(\"Are there any first-class flights at 11pm for less than $3.50?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b7f36c",
      "metadata": {
        "id": "a1b7f36c"
      },
      "outputs": [],
      "source": [
        "## Test sentence\n",
        "test_sentence_1 = tokenize(\"show me the flights before noon\")\n",
        "\n",
        "## Construct parser from distribution grammar\n",
        "atis_parser_distrib = nltk.parse.BottomUpChartParser(atis_grammar_distrib)\n",
        "\n",
        "## Parse and print the parses\n",
        "parses = atis_parser_distrib.parse(test_sentence_1)\n",
        "for parse in parses:\n",
        "  parse.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11849917",
      "metadata": {
        "id": "11849917"
      },
      "source": [
        "## Testing the coverage of the grammar\n",
        "\n",
        "We can get a sense of how well the grammar covers the ATIS query language by measuring the proportion of queries in the training set that are parsable by the grammar. We define a `coverage` function to carry out this evaluation.\n",
        "\n",
        "> Warning: It may take a long time to parse all of the sentence in the training corpus, on the order of 30 minutes. You may want to start with just the first few sentences in the corpus. The `coverage` function below makes it easy to do so, and in the code below we just test coverage on the first 50 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "3b73d5af",
      "metadata": {
        "id": "3b73d5af"
      },
      "outputs": [],
      "source": [
        "## Read in the training corpus\n",
        "with open('data/train.nl') as file:\n",
        "  training_corpus = [tokenize(line) for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "f3185ce8",
      "metadata": {
        "id": "f3185ce8"
      },
      "outputs": [],
      "source": [
        "def coverage(recognizer, corpus, n=0):\n",
        "  \"\"\"Returns the proportion of the first `n` sentences in the `corpus`\n",
        "  that are recognized by the `recognizer`, which should return a boolean.\n",
        "  `n` is taken to be the whole corpus if n is not provided or is\n",
        "  non-positive.\n",
        "  \"\"\"\n",
        "  n = len(corpus) if n <= 0 else n\n",
        "  parsed = 0\n",
        "  total = 0\n",
        "  for sent in tqdm(corpus[:n]):\n",
        "    total += 1\n",
        "    try:\n",
        "      parses = recognizer(sent)\n",
        "    except:\n",
        "      parses = None\n",
        "    if parses:\n",
        "      parsed += 1\n",
        "    elif DEBUG:\n",
        "      print(f\"failed: {sent}\")\n",
        "  if DEBUG: print(f\"{parsed} of {total}\")\n",
        "  return parsed/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "493eac20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "493eac20",
        "outputId": "4e5ce911-bebc-44f5-87e0-58afdf222547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 194.38it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "coverage(lambda sent: 0 < len(list(atis_parser_distrib.parse(sent))),  # trick for turning parser into recognizer\n",
        "         training_corpus, n=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c1d48e",
      "metadata": {
        "id": "92c1d48e"
      },
      "source": [
        "Sadly, you'll find that the coverage of the grammar is extraordinarily poor. That's because it is missing crucial parts of the grammar, especially phrases about _places_, which play a role in essentially every ATIS query. You'll need to complete the grammar before it can be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5a1e7d",
      "metadata": {
        "id": "4f5a1e7d"
      },
      "source": [
        "## Part 1: Finish the CFG for the ATIS dataset\n",
        "\n",
        "Consider the following query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "688a21c4",
      "metadata": {
        "id": "688a21c4"
      },
      "outputs": [],
      "source": [
        "test_sentence_2 = tokenize(\"show me the united flights from boston\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "799f2fbc",
      "metadata": {
        "id": "799f2fbc"
      },
      "source": [
        "You'll notice that the grammar we distributed doesn't handle this query because it doesn't have a subgrammar for airline information (`\"united\"`) or for places (`\"from boston\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "57f11593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57f11593",
        "outputId": "8b704fea-e9d4-40e8-aa33-16ac30558ea5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "len(list(atis_parser_distrib.parse(test_sentence_2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3f533d",
      "metadata": {
        "id": "1d3f533d"
      },
      "source": [
        "Follow the instructions in the grammar file `data/grammar` to add further coverage to the grammar. (You can and should leave the `data/grammar_distrib3` copy alone and use it for reference.)\n",
        "\n",
        "We'll define a parser based on your modified grammar, so we can compare it against the distributed grammar. Once you've modified the grammar, this test sentence should have at least one parse.\n",
        "\n",
        "> You can search for \"TODO\" in `data/grammar` to find the two places to add grammar rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "6cbb8e3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbb8e3e",
        "outputId": "dce445de-af5e-43f3-e566-f6314764a1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                S                                                            \n",
            "                        ________________________________________|_______________________                                      \n",
            "                       |                                                            NP_FLIGHT                                \n",
            "                       |                                                                |                                     \n",
            "                       |                                                            NOM_FLIGHT                               \n",
            "                       |                                         _______________________|__________                           \n",
            "                       |                                        |                              NOM_FLIGHT                    \n",
            "                       |                                        |                                  |                          \n",
            "                   PREIGNORE                                    |                               N_FLIGHT                     \n",
            "        _______________|____________                            |             _____________________|_________                 \n",
            "       |                        PREIGNORE                      ADJ           |                               PP              \n",
            "       |                ____________|____________               |            |                               |                \n",
            "       |               |                     PREIGNORE     ADJ_AIRLINE    N_FLIGHT                        PP_PLACE           \n",
            "       |               |                         |              |            |                      _________|_________       \n",
            "PREIGNORESYMBOL PREIGNORESYMBOL           PREIGNORESYMBOL TERM_AIRBRAND TERM_FLIGHT             P_PLACE            TERM_PLACE\n",
            "       |               |                         |              |            |                     |                   |      \n",
            "      show             me                       the           united      flights                 from               boston  \n",
            "\n",
            "                                                    S                                                            \n",
            "                     _______________________________|____________                                                 \n",
            "                    |                                        NP_FLIGHT                                           \n",
            "                    |                       _____________________|__________                                      \n",
            "                    |                      |                            NOM_FLIGHT                               \n",
            "                    |                      |         _______________________|__________                           \n",
            "                    |                      |        |                              NOM_FLIGHT                    \n",
            "                    |                      |        |                                  |                          \n",
            "                    |                      |        |                               N_FLIGHT                     \n",
            "                    |                      |        |             _____________________|_________                 \n",
            "                PREIGNORE                  |       ADJ           |                               PP              \n",
            "        ____________|____________          |        |            |                               |                \n",
            "       |                     PREIGNORE     |   ADJ_AIRLINE    N_FLIGHT                        PP_PLACE           \n",
            "       |                         |         |        |            |                      _________|_________       \n",
            "PREIGNORESYMBOL           PREIGNORESYMBOL DET TERM_AIRBRAND TERM_FLIGHT             P_PLACE            TERM_PLACE\n",
            "       |                         |         |        |            |                     |                   |      \n",
            "      show                       me       the     united      flights                 from               boston  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "atis_grammar_expanded, _ = xform.read_augmented_grammar(\"grammar\", path=\"data\")\n",
        "atis_parser_expanded = nltk.parse.BottomUpChartParser(atis_grammar_expanded)\n",
        "\n",
        "parses = [p for p in atis_parser_expanded.parse(test_sentence_2)]\n",
        "for parse in parses:\n",
        "  parse.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "a857aabd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a857aabd",
        "outputId": "3f281bd5-613a-437c-d3e7-2588217ed0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                 S                                                                                                      \n",
            "                        _________________________________________________________________________________________|__________                                                                                             \n",
            "                       |                                                                                                NP_FLIGHT                                                                                       \n",
            "                       |                                                                                                    |                                                                                            \n",
            "                       |                                                                                                NOM_FLIGHT                                                                                      \n",
            "                       |                                            ________________________________________________________|___________________                                                                         \n",
            "                       |                                           |                                                                        NOM_FLIGHT                                                                  \n",
            "                       |                                           |                           _________________________________________________|__________                                                              \n",
            "                       |                                           |                          |                                                        NOM_FLIGHT                                                       \n",
            "                       |                                           |                          |                                                            |                                                             \n",
            "                       |                                           |                          |                                                         N_FLIGHT                                                        \n",
            "                       |                                           |                          |                                                  __________|____________________________________                         \n",
            "                       |                                           |                          |                                              N_FLIGHT                                           |                       \n",
            "                       |                                           |                          |                              ___________________|____________________                           |                        \n",
            "                   PREIGNORE                                       |                          |                          N_FLIGHT                                    |                          |                       \n",
            "        _______________|____________                               |                          |                   __________|_________                               |                          |                        \n",
            "       |                        PREIGNORE                          |                         ADJ                 |                    PP                             PP                         PP                      \n",
            "       |                ____________|____________                  |                          |                  |                    |                              |                          |                        \n",
            "       |               |                     PREIGNORE            ADJ                   ADJ_FLIGHTTYPE        N_FLIGHT             PP_PLACE                       PP_PLACE                  PP_AIRLINE                  \n",
            "       |               |                         |                 |                          |                  |           _________|_________            _________|_________         ________|______________          \n",
            "PREIGNORESYMBOL PREIGNORESYMBOL           PREIGNORESYMBOL      ADJ_PRICE               ADJ_FLIGHTTYPESI     TERM_FLIGHT  P_PLACE            TERM_PLACE  P_PLACE            TERM_PLACE  |  TERM_AIRBRAND TERM_AIRBRANDTYP\n",
            "       |               |                         |                 |                         MPLE                |          |                   |          |                   |       |        |              E        \n",
            "       |               |                         |          _______|_________        _________|__________        |          |                   |          |                   |       |        |              |         \n",
            "      what             is                       the       most           expensive one                  way    flight      from               boston       to               atlanta    on    american       airlines    \n",
            "\n",
            "                                           S                                                                                                                                                                      \n",
            "                     ______________________|_______________________________                                                                                                                                        \n",
            "                    |                                                  NP_FLIGHT                                                                                                                                  \n",
            "                    |                       _______________________________|__________________________________________                                                                                             \n",
            "                    |                      |                                                                      NOM_FLIGHT                                                                                      \n",
            "                    |                      |            ______________________________________________________________|___________________                                                                         \n",
            "                    |                      |           |                                                                              NOM_FLIGHT                                                                  \n",
            "                    |                      |           |                                 _________________________________________________|__________                                                              \n",
            "                    |                      |           |                                |                                                        NOM_FLIGHT                                                       \n",
            "                    |                      |           |                                |                                                            |                                                             \n",
            "                    |                      |           |                                |                                                         N_FLIGHT                                                        \n",
            "                    |                      |           |                                |                                                  __________|____________________________________                         \n",
            "                    |                      |           |                                |                                              N_FLIGHT                                           |                       \n",
            "                    |                      |           |                                |                              ___________________|____________________                           |                        \n",
            "                    |                      |           |                                |                          N_FLIGHT                                    |                          |                       \n",
            "                    |                      |           |                                |                   __________|_________                               |                          |                        \n",
            "                PREIGNORE                  |           |                               ADJ                 |                    PP                             PP                         PP                      \n",
            "        ____________|____________          |           |                                |                  |                    |                              |                          |                        \n",
            "       |                     PREIGNORE     |          ADJ                         ADJ_FLIGHTTYPE        N_FLIGHT             PP_PLACE                       PP_PLACE                  PP_AIRLINE                  \n",
            "       |                         |         |           |                                |                  |           _________|_________            _________|_________         ________|______________          \n",
            "PREIGNORESYMBOL           PREIGNORESYMBOL DET      ADJ_PRICE                     ADJ_FLIGHTTYPESI     TERM_FLIGHT  P_PLACE            TERM_PLACE  P_PLACE            TERM_PLACE  |  TERM_AIRBRAND TERM_AIRBRANDTYP\n",
            "       |                         |         |           |                               MPLE                |          |                   |          |                   |       |        |              E        \n",
            "       |                         |         |    _______|_________           ____________|__________        |          |                   |          |                   |       |        |              |         \n",
            "      what                       is       the most           expensive    one                     way    flight      from               boston       to               atlanta    on    american       airlines    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_sentence_3 = tokenize(\"what is the most expensive one way flight from boston to atlanta on american airlines\")\n",
        "parses = [p for p in atis_parser_expanded.parse(test_sentence_3)]\n",
        "for parse in parses:\n",
        "  parse.pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72531490",
      "metadata": {
        "id": "72531490"
      },
      "source": [
        "Once you're done adding to the grammar, to check your grammar, we'll compute the grammar's coverage of the ATIS training corpus as before.\n",
        "**This grammar should be expected to cover about half of the sentences in the first 50 sentences, and a third of the entire training corpus.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "bef9bbe1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bef9bbe1",
        "outputId": "66376538-1f41-464d-a4db-2a5de06762ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 421.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed: ['what', 'is', 'the', 'most', 'expensive', 'one', 'way', 'fare', 'from', 'boston', 'to', 'atlanta', 'on', 'american', 'airlines']\n",
            "failed: ['i', \"'d\", 'like', 'to', 'see', 'flights', 'from', 'baltimore', 'to', 'atlanta', 'that', 'arrive', 'before', 'noon', 'and', 'i', \"'d\", 'like', 'to', 'see', 'flights', 'from', 'denver', 'to', 'atlanta', 'that', 'arrive', 'before', 'noon']\n",
            "failed: ['do', 'you', 'have', 'an', '819', 'flight', 'from', 'denver', 'to', 'san', 'francisco']\n",
            "failed: ['i', 'would', 'like', 'a', 'us', 'air', 'flight', 'from', 'toronto', 'to', 'san', 'diego', 'with', 'a', 'stopover', 'in', 'denver', 'please']\n",
            "failed: ['how', 'can', 'i', 'get', 'from', 'boston', 'to', 'atlanta', 'and', 'back', 'in', 'the', 'same', 'day', 'and', 'have', 'the', 'most', 'hours', 'on', 'the', 'ground', 'in', 'atlanta']\n",
            "failed: ['are', 'there', 'any', 'flights', 'between', 'pittsburgh', 'and', 'baltimore', 'using', 'a', 'j31', 'aircraft']\n",
            "failed: ['what', 'does', 'd', '/s', 'stand', 'for', 'for', 'meals']\n",
            "failed: ['pittsburgh', 'to', 'boston', 'saturday']\n",
            "failed: ['i', 'need', 'the', 'cost', 'of', 'a', 'ticket', 'going', 'from', 'denver', 'to', 'baltimore', 'a', 'first', 'class', 'ticket', 'on', 'united', 'airlines']\n",
            "failed: ['list', 'the', 'takeoffs', 'and', 'landings', 'at', 'general', 'mitchell', 'international']\n",
            "failed: ['what', 'flights', 'are', 'there', 'from', 'minneapolis', 'to', 'newark', 'on', 'continental']\n",
            "failed: ['is', 'there', 'limo', 'service', 'at', 'pittsburgh', 'airport']\n",
            "failed: ['how', 'much', 'is', 'a', 'first', 'class', 'ticket', 'from', 'boston', 'to', 'san', 'francisco']\n",
            "failed: ['in', 'dallas', 'fort', 'worth', 'i', 'would', 'like', 'information', 'on', 'ground', 'transportation']\n",
            "failed: ['i', \"'d\", 'like', 'flights', 'on', 'american', 'airlines', 'from', 'philadelphia', 'philadelphia', 'to', 'dallas', 'arriving', 'before', '1145', 'am']\n",
            "failed: ['what', 'airlines', 'from', 'washington', 'dc', 'to', 'columbus']\n",
            "failed: ['what', 'flights', 'do', 'you', 'have', 'in', 'the', 'morning', 'of', 'september', 'twentieth', 'on', 'united', 'airlines', 'from', 'pittsburgh', 'to', 'san', 'francisco', 'and', 'a', 'stopover', 'in', 'denver']\n",
            "failed: ['what', 'is', 'the', 'cost', 'of', 'a', 'round', 'trip', 'flight', 'from', 'pittsburgh', 'to', 'atlanta', 'beginning', 'on', 'april', 'twenty', 'fifth', 'and', 'returning', 'on', 'may', 'sixth']\n",
            "failed: ['i', 'would', 'like', 'a', 'flight', 'from', 'washington', 'to', 'boston', 'leaving', 'at', '230', 'on', 'august', 'twentieth']\n",
            "failed: ['i', \"'d\", 'like', 'to', 'fly', 'nonstop', 'from', 'atlanta', 'to', 'baltimore', 'and', 'get', 'there', 'at', '7', 'pm']\n",
            "failed: ['what', 'is', 'the', 'type', 'of', 'aircraft', 'for', 'united', 'flight', '21']\n",
            "failed: ['can', 'you', 'give', 'me', 'information', 'on', 'transportation', 'from', 'the', 'airport', 'in', 'philadelphia', 'to', 'downtown', 'philadelphia']\n",
            "failed: ['what', 'is', 'the', 'fare', 'going', 'from', 'baltimore', 'to', 'boston', 'one', 'way', 'on', 'november', 'seventh']\n",
            "failed: ['on', 'usa', 'air', 'how', 'many', 'flights', 'leaving', 'oakland', 'on', 'july', 'twenty', 'seventh', 'to', 'boston', 'nonstop']\n",
            "failed: ['show', 'me', 'the', 'first', 'class', 'fares', 'from', 'baltimore', 'to', 'dallas']\n",
            "failed: ['what', 'flights', 'do', 'you', 'have', 'from', 'burbank', 'to', 'tacoma', 'washington']\n",
            "24 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "coverage(lambda sent: 0 < len(list(atis_parser_expanded.parse(sent))),  # trick for turning parser into recognizer\n",
        "         training_corpus, n=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33adb1d2",
      "metadata": {
        "id": "33adb1d2"
      },
      "source": [
        "# CFG recognition via the CKY algorithm\n",
        "\n",
        "Now we turn to implementing recognizers and parsers using the CKY algorithm. We start with a recognizer, which should return `True` or `False` if a grammar does or does not admit a sentence as grammatical.\n",
        "\n",
        "## Converting the grammar to CNF for use by the CKY algorithm\n",
        "\n",
        "The CKY algorithm requires the grammar to be in Chomsky normal form (CNF). That is, only rules of the forms\n",
        "\\begin{align*}\n",
        "A &\\rightarrow B\\, C\\\\\n",
        "A &\\rightarrow a\n",
        "\\end{align*}\n",
        "are allowed, where $A$, $B$, $C$ are nonterminals and $a$ is a terminal symbol.\n",
        "\n",
        "However, in some downstream applications (such as the next project segment) we want to use grammar rules of more general forms, such as $A \\rightarrow B\\, C\\, D$. Indeed, the ATIS grammar you've been working on makes use of the additional expressivity beyond CNF.\n",
        "\n",
        "To satisfy both of these constraints, we will convert the grammar to CNF, parse using CKY, and then convert the returned parse trees back to the form of the original grammar. We provide some useful functions for performing these transformations in the file `scripts/transform.py`, already loaded above and imported as `xform`.\n",
        "\n",
        "To convert a grammar to CNF:\n",
        "\n",
        "`cnf_grammar, cnf_grammar_wunaries = xform.get_cnf_grammar(grammar)`\n",
        "\n",
        "To convert a tree output from CKY back to the original form of the grammar:\n",
        "\n",
        "`xform.un_cnf(tree, cnf_grammar_wunaries)`\n",
        "\n",
        "> We pass into `un_cnf` a version of the grammar before removing unary nonterminal productions, `cnf_grammar_wunaries`. The `cnf_grammar_wunaries` is returened as the second part of the returned value of `get_cnf_grammar` for just this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "c919b89e",
      "metadata": {
        "id": "c919b89e"
      },
      "outputs": [],
      "source": [
        "atis_grammar_cnf, atis_grammar_wunaries = xform.get_cnf_grammar(atis_grammar_expanded)\n",
        "assert(atis_grammar_cnf.is_chomsky_normal_form())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef19966d",
      "metadata": {
        "id": "ef19966d"
      },
      "source": [
        "In the next sections, you'll write your own recognizers and parsers based on the CKY algorithm that can operate on this grammar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380a77f5",
      "metadata": {
        "id": "380a77f5"
      },
      "source": [
        "## Part 2: Implement a CKY recognizer\n",
        "\n",
        "Implement a _recognizer_ using the CKY algorithm to determine if a sentence `tokens` is parsable. The labs and J&M Chapter 13, both of which provide appropriaste pseudo-code for CKY, should be useful references here.\n",
        "\n",
        "> **Hint:** Recall that you can get the production rules of a grammar using `grammar.productions()`.\n",
        "\n",
        "> Throughtout this project segment, you should use `grammar.start()` to get the special start symbol from the grammar instead of using `S`, since some grammar uses a different start symbol, such as `TOP`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "355a3d12",
      "metadata": {
        "id": "355a3d12"
      },
      "outputs": [],
      "source": [
        "## TODO – Implement a CKY recognizer\n",
        "import numpy as np\n",
        "def cky_recognize(grammar, tokens):\n",
        "    \"\"\"Returns True if and only if the list of tokens `tokens` is admitted\n",
        "    by the `grammar`.\n",
        "\n",
        "    Implements the CKY algorithm, and therefore assumes `grammar` is in\n",
        "    Chomsky normal form.\n",
        "    \"\"\"\n",
        "    #print(grammar.productions())\n",
        "    tokens=[\"\"]+tokens\n",
        "    assert(grammar.is_chomsky_normal_form())\n",
        "    N=len(tokens)+1\n",
        "    table_data = [[set() for i in range(N+1)] for j in range(N+1)]\n",
        "    for index,value in enumerate(tokens):\n",
        "        #print(value)\n",
        "        terminal=terminal_finder(grammar,value)\n",
        "        if terminal is not None:\n",
        "          table_data[index-1][index].add(terminal.lhs())\n",
        "    for j in range(1,N+1):\n",
        "        for length in range(2,j+1):\n",
        "            i=j-length\n",
        "\n",
        "            for split in range(1+i,j):\n",
        "                B= table_data[i][split]\n",
        "                C=table_data[split][j]\n",
        "                table_data[i][j]=united_not_airlines(grammar,B,C)\n",
        "    #print(table_data)\n",
        "    return grammar.start() == table_data[0][N-1]\n",
        "\n",
        "def terminal_finder(grammar,terminal):\n",
        "  for rule in grammar.productions():\n",
        "    #print(rule.rhs()[0],terminal)\n",
        "    if rule.rhs()[0] == terminal:\n",
        "      return rule\n",
        "\n",
        "def united_not_airlines(grammar,B,C):\n",
        "  united=set()\n",
        "  for b in B:\n",
        "    for c in C:\n",
        "      for rule in grammar.productions():\n",
        "\n",
        "        if len(rule.rhs()) == 1:\n",
        "            continue\n",
        "        #print(rule.rhs()[0],rule.rhs()[1])\n",
        "        #print(b,c)\n",
        "        if b in str(rule.rhs()[0]) and c in str(rule.rhs()[1]):\n",
        "\n",
        "            united.add(rule.lhs())\n",
        "  return united"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc8eae7",
      "metadata": {
        "id": "8bc8eae7"
      },
      "source": [
        "You can test your recognizer on a few examples, both grammatical and ungrammatical, as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "3bd2ab73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "6a5ea1da-1dc0-421c-e3c2-26bdab85f53f",
        "id": "3bd2ab73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S -> PREIGNORE NP_FLIGHT, S -> NP_FLIGHT POSTIGNORE, S -> PREIGNORE S|<NP_FLIGHT-POSTIGNORE>, S|<NP_FLIGHT-POSTIGNORE> -> NP_FLIGHT POSTIGNORE, NP_FLIGHT -> DET NOM_FLIGHT, NOM_FLIGHT -> ADJ NOM_FLIGHT, N_FLIGHT -> N_FLIGHT PP, TERM_FLIGHT -> 'flights', TERM_FLIGHT -> 'flight', $to -> 'to', $fly -> 'fly', TERM_FLIGHT -> $to $fly, $all -> 'all', $the -> 'the', DET -> $all $the, DET -> 'all', DET -> 'a', DET -> 'an', DET -> 'the', DET -> 'any', $all -> 'all', DET -> $all DET|<@of-@the>, $of -> 'of', $the -> 'the', DET|<@of-@the> -> $of $the, DET -> 'this', $this -> 'this', $coming -> 'coming', DET -> $this $coming, DET -> 'next', $'s -> \"'s\", DET -> TERM_WEEKDAY $'s, $on -> 'on', PP_AIRLINE -> $on PP_AIRLINE|<TERM_AIRBRAND-TERM_AIRBRANDTYPE>, PP_AIRLINE|<TERM_AIRBRAND-TERM_AIRBRANDTYPE> -> TERM_AIRBRAND TERM_AIRBRANDTYPE, $on -> 'on', PP_AIRLINE -> $on TERM_AIRBRAND, TERM_AIRBRAND -> 'continental', TERM_AIRBRAND -> 'american', TERM_AIRBRAND -> 'united', TERM_AIRBRAND -> 'northwest', TERM_AIRBRAND -> 'us', TERM_AIRBRAND -> 'delta', TERM_AIRBRAND -> 'twa', $air -> 'air', $canada -> 'canada', TERM_AIRBRAND -> $air $canada, $midwest -> 'midwest', $express -> 'express', TERM_AIRBRAND -> $midwest $express, $trans -> 'trans', $world -> 'world', TERM_AIRBRAND -> $trans $world, TERM_AIRBRANDTYPE -> 'airline', TERM_AIRBRANDTYPE -> 'airlines', TERM_AIRBRANDTYPE -> 'air', ADJ_FLIGHTTYPE -> ADJ_FLIGHTTYPESIMPLE ADJ_FLIGHTTYPE|<@and-ADJ_FLIGHTTYPE>, $and -> 'and', ADJ_FLIGHTTYPE|<@and-ADJ_FLIGHTTYPE> -> $and ADJ_FLIGHTTYPE, ADJ_FLIGHTTYPE -> ADJ_FLIGHTTYPESIMPLE ADJ_FLIGHTTYPE|<@or-ADJ_FLIGHTTYPE>, $or -> 'or', ADJ_FLIGHTTYPE|<@or-ADJ_FLIGHTTYPE> -> $or ADJ_FLIGHTTYPE, $round -> 'round', $trip -> 'trip', ADJ_FLIGHTTYPESIMPLE -> $round $trip, ADJ_FLIGHTTYPESIMPLE -> 'round-trip', ADJ_FLIGHTTYPESIMPLE -> 'return', $one -> 'one', $way -> 'way', ADJ_FLIGHTTYPESIMPLE -> $one $way, ADJ_FLIGHTTYPESIMPLE -> 'nonstop', ADJ_FLIGHTTYPESIMPLE -> 'direct', ADJ_FLIGHTTYPESIMPLE -> 'connecting', ADJ_PRICE -> 'cheapest', $lowest -> 'lowest', $cost -> 'cost', ADJ_PRICE -> $lowest $cost, $least -> 'least', $expensive -> 'expensive', ADJ_PRICE -> $least $expensive, ADJ_PRICE -> 'inexpensive', ADJ_PRICE -> 'cheap', ADJ_PRICE -> 'expensive', $highest -> 'highest', $cost -> 'cost', ADJ_PRICE -> $highest $cost, $most -> 'most', $expensive -> 'expensive', ADJ_PRICE -> $most $expensive, $less -> 'less', PP_PRICE -> $less PP_PRICE|<@than-AMOUNT-@dollars>, $than -> 'than', PP_PRICE|<@than-AMOUNT-@dollars> -> $than PP_PRICE|<AMOUNT-@dollars>, $dollars -> 'dollars', PP_PRICE|<AMOUNT-@dollars> -> AMOUNT $dollars, $with -> 'with', PP_PRICE -> $with PP_PRICE|<@the-@lowest-@fare>, $the -> 'the', PP_PRICE|<@the-@lowest-@fare> -> $the PP_PRICE|<@lowest-@fare>, $lowest -> 'lowest', $fare -> 'fare', PP_PRICE|<@lowest-@fare> -> $lowest $fare, $the -> 'the', PP_PRICE -> $the PP_PRICE|<@cheapest-@way-@possible>, $cheapest -> 'cheapest', PP_PRICE|<@cheapest-@way-@possible> -> $cheapest PP_PRICE|<@way-@possible>, $way -> 'way', $possible -> 'possible', PP_PRICE|<@way-@possible> -> $way $possible, $with -> 'with', PP_PRICE -> $with PP_PRICE|<@the-@highest-@fare>, $the -> 'the', PP_PRICE|<@the-@highest-@fare> -> $the PP_PRICE|<@highest-@fare>, $highest -> 'highest', $fare -> 'fare', PP_PRICE|<@highest-@fare> -> $highest $fare, ADJ_CLASS -> 'economy', $thrift -> 'thrift', $economy -> 'economy', ADJ_CLASS -> $thrift $economy, $first -> 'first', $class -> 'class', ADJ_CLASS -> $first $class, ADJ_CLASS -> 'transcontinental', ADJ_CLASS -> 'available', ADJ_CLASS -> 'possible', PP_FOOD -> P_FOOD NP_FOOD, NP_FOOD -> 'dinner', NP_FOOD -> 'lunch', NP_FOOD -> 'breakfast', $a -> 'a', $meal -> 'meal', NP_FOOD -> $a $meal, P_FOOD -> 'serving', P_FOOD -> 'with', PP_DATE -> P_DATE NP_DATE, P_DATE -> 'on', $returning -> 'returning', $on -> 'on', P_DATE -> $returning $on, P_DATE -> 'of', P_DATE -> 'for', P_DATE -> 'next', $the -> 'the', $next -> 'next', P_DATE -> $the $next, $in -> 'in', P_DATE -> $in P_DATE|<@the-@next>, $the -> 'the', $next -> 'next', P_DATE|<@the-@next> -> $the $next, $of -> 'of', $next -> 'next', P_DATE -> $of $next, P_DATE -> 'leaving', P_DATE -> 'departing', $departing -> 'departing', $on -> 'on', P_DATE -> $departing $on, $which -> 'which', $leave -> 'leave', P_DATE -> $which $leave, $leaving -> 'leaving', $on -> 'on', P_DATE -> $leaving $on, P_DATE -> 'arriving', $arriving -> 'arriving', $on -> 'on', P_DATE -> $arriving $on, $that -> 'that', P_DATE -> $that P_DATE|<@arrive-@on>, $arrive -> 'arrive', $on -> 'on', P_DATE|<@arrive-@on> -> $arrive $on, $which -> 'which', P_DATE -> $which P_DATE|<@arrive-@on>, $arrive -> 'arrive', $on -> 'on', P_DATE|<@arrive-@on> -> $arrive $on, $a -> 'a', P_DATE -> $a P_DATE|<@week-@from>, $week -> 'week', $from -> 'from', P_DATE|<@week-@from> -> $week $from, $a -> 'a', NP_DATE -> $a TERM_WEEKDAY, $this -> 'this', NP_DATE -> $this TERM_WEEKDAY, $this -> 'this', $coming -> 'coming', NP_DATE -> $this $coming, NP_DATE -> TERM_WEEKDAY ADJ_TIME, NP_DATE -> 'saturdays', NP_DATE -> 'sundays', NP_DATE -> 'mondays', NP_DATE -> 'tuesdays', NP_DATE -> 'wednesdays', NP_DATE -> 'thursdays', NP_DATE -> 'fridays', NP_DATE -> 'day', NP_DATE -> 'week', NP_DATE -> 'today', NP_DATE -> 'tomorrow', $the -> 'the', NP_DATE -> $the NP_DATE|<@day-@after-@tomorrow>, $day -> 'day', NP_DATE|<@day-@after-@tomorrow> -> $day NP_DATE|<@after-@tomorrow>, $after -> 'after', $tomorrow -> 'tomorrow', NP_DATE|<@after-@tomorrow> -> $after $tomorrow, NP_DATE -> 'weekdays', NP_MDYDATE -> TERM_MONTH NP_MDYDATE|<TERM_DAY-YEAR>, NP_MDYDATE|<TERM_DAY-YEAR> -> TERM_DAY YEAR, NP_MDYDATE -> TERM_MONTH TERM_DAY, $the -> 'the', NP_MDYDATE -> $the TERM_DAY, NP_MDYDATE -> TERM_MONTH NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, NP_MDYDATE|<TERM_DAY-@or-TERM_DAY> -> TERM_DAY NP_MDYDATE|<@or-TERM_DAY>, $or -> 'or', NP_MDYDATE|<@or-TERM_DAY> -> $or TERM_DAY, $the -> 'the', NP_MDYDATE -> $the NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, NP_MDYDATE|<TERM_DAY-@or-TERM_DAY> -> TERM_DAY NP_MDYDATE|<@or-TERM_DAY>, $or -> 'or', NP_MDYDATE|<@or-TERM_DAY> -> $or TERM_DAY, $either -> 'either', NP_MDYDATE -> $either NP_MDYDATE|<TERM_MONTH-TERM_DAY-@or-TERM_DAY>, NP_MDYDATE|<TERM_MONTH-TERM_DAY-@or-TERM_DAY> -> TERM_MONTH NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, NP_MDYDATE|<TERM_DAY-@or-TERM_DAY> -> TERM_DAY NP_MDYDATE|<@or-TERM_DAY>, $or -> 'or', NP_MDYDATE|<@or-TERM_DAY> -> $or TERM_DAY, $either -> 'either', NP_MDYDATE -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY>, $the -> 'the', NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY> -> $the NP_MDYDATE|<TERM_DAY-@or-@the-TERM_DAY>, NP_MDYDATE|<TERM_DAY-@or-@the-TERM_DAY> -> TERM_DAY NP_MDYDATE|<@or-@the-TERM_DAY>, $or -> 'or', NP_MDYDATE|<@or-@the-TERM_DAY> -> $or NP_MDYDATE|<@the-TERM_DAY>, $the -> 'the', NP_MDYDATE|<@the-TERM_DAY> -> $the TERM_DAY, $the -> 'the', NP_MDYDATE -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH>, NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH> -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH> -> $of TERM_MONTH, NP_MDYDATE -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH> -> $of TERM_MONTH, $either -> 'either', NP_MDYDATE -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH>, $the -> 'the', NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH> -> $the NP_MDYDATE|<TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH>, NP_MDYDATE|<TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH> -> TERM_DAY NP_MDYDATE|<@or-@the-TERM_DAY-@of-TERM_MONTH>, $or -> 'or', NP_MDYDATE|<@or-@the-TERM_DAY-@of-TERM_MONTH> -> $or NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH>, $the -> 'the', NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH> -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH>, NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH> -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH> -> $of TERM_MONTH, $the -> 'the', NP_MDYDATE -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH>, NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH> -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH> -> $of NP_MDYDATE|<TERM_MONTH-@or-@the-@day-@of-TERM_MONTH>, NP_MDYDATE|<TERM_MONTH-@or-@the-@day-@of-TERM_MONTH> -> TERM_MONTH NP_MDYDATE|<@or-@the-@day-@of-TERM_MONTH>, $or -> 'or', NP_MDYDATE|<@or-@the-@day-@of-TERM_MONTH> -> $or NP_MDYDATE|<@the-@day-@of-TERM_MONTH>, $the -> 'the', NP_MDYDATE|<@the-@day-@of-TERM_MONTH> -> $the NP_MDYDATE|<@day-@of-TERM_MONTH>, $day -> 'day', NP_MDYDATE|<@day-@of-TERM_MONTH> -> $day NP_MDYDATE|<@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH> -> $of TERM_MONTH, $either -> 'either', NP_MDYDATE -> $either NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, $the -> 'the', NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH> -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH> -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH> -> $of NP_MDYDATE|<TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, NP_MDYDATE|<TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH> -> TERM_MONTH NP_MDYDATE|<@or-@the-TERM_DAY-@of-TERM_MONTH>, $or -> 'or', NP_MDYDATE|<@or-@the-TERM_DAY-@of-TERM_MONTH> -> $or NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH>, $the -> 'the', NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH> -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH>, NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH> -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, $of -> 'of', NP_MDYDATE|<@of-TERM_MONTH> -> $of TERM_MONTH, TERM_WEEKDAY -> 'saturday', TERM_WEEKDAY -> 'sunday', TERM_WEEKDAY -> 'monday', TERM_WEEKDAY -> 'tuesday', TERM_WEEKDAY -> 'wednesday', TERM_WEEKDAY -> 'thursday', TERM_WEEKDAY -> 'friday', TERM_WEEKDAY -> 'weekday', TERM_MONTH -> 'january', TERM_MONTH -> 'february', TERM_MONTH -> 'march', TERM_MONTH -> 'april', TERM_MONTH -> 'may', TERM_MONTH -> 'june', TERM_MONTH -> 'july', TERM_MONTH -> 'august', TERM_MONTH -> 'september', TERM_MONTH -> 'october', TERM_MONTH -> 'november', TERM_MONTH -> 'december', TERM_DAY -> 'first', TERM_DAY -> 'second', TERM_DAY -> 'third', TERM_DAY -> 'fourth', TERM_DAY -> 'fifth', TERM_DAY -> 'sixth', TERM_DAY -> 'seventh', TERM_DAY -> 'eighth', TERM_DAY -> 'ninth', TERM_DAY -> 'tenth', TERM_DAY -> 'eleventh', TERM_DAY -> 'twelfth', TERM_DAY -> 'thirteenth', TERM_DAY -> 'fourteenth', TERM_DAY -> 'fifteenth', TERM_DAY -> 'sixteenth', TERM_DAY -> 'seventeenth', TERM_DAY -> 'eighteenth', TERM_DAY -> 'nineteenth', TERM_DAY -> 'twentieth', TERM_DAY -> 'twenty-first', $twenty -> 'twenty', $first -> 'first', TERM_DAY -> $twenty $first, TERM_DAY -> 'twenty-second', $twenty -> 'twenty', $second -> 'second', TERM_DAY -> $twenty $second, TERM_DAY -> 'twenty-third', $twenty -> 'twenty', $third -> 'third', TERM_DAY -> $twenty $third, TERM_DAY -> 'twenty-fourth', $twenty -> 'twenty', $fourth -> 'fourth', TERM_DAY -> $twenty $fourth, TERM_DAY -> 'twenty-fifth', $twenty -> 'twenty', $fifth -> 'fifth', TERM_DAY -> $twenty $fifth, TERM_DAY -> 'twenty-sixth', $twenty -> 'twenty', $sixth -> 'sixth', TERM_DAY -> $twenty $sixth, TERM_DAY -> 'twenty-seventh', $twenty -> 'twenty', $seventh -> 'seventh', TERM_DAY -> $twenty $seventh, TERM_DAY -> 'twenty-eighth', $twenty -> 'twenty', $eighth -> 'eighth', TERM_DAY -> $twenty $eighth, TERM_DAY -> 'twenty-ninth', $twenty -> 'twenty', $ninth -> 'ninth', TERM_DAY -> $twenty $ninth, TERM_DAY -> 'thirtieth', TERM_DAY -> 'thirty-first', $thirty -> 'thirty', $first -> 'first', TERM_DAY -> $thirty $first, TERM_DAY -> 'one', TERM_DAY -> 'two', TERM_DAY -> 'three', TERM_DAY -> 'four', TERM_DAY -> 'five', TERM_DAY -> 'six', TERM_DAY -> 'seven', TERM_DAY -> 'eight', TERM_DAY -> 'nine', TERM_DAY -> 'ten', TERM_DAY -> 'eleven', TERM_DAY -> 'twelve', TERM_DAY -> 'thirteen', TERM_DAY -> 'fourteen', TERM_DAY -> 'fifteen', TERM_DAY -> 'sixteen', TERM_DAY -> 'seventeen', TERM_DAY -> 'eighteen', TERM_DAY -> 'nineteen', TERM_DAY -> 'twenty', $twenty -> 'twenty', $one -> 'one', TERM_DAY -> $twenty $one, $twenty -> 'twenty', $two -> 'two', TERM_DAY -> $twenty $two, $twenty -> 'twenty', $three -> 'three', TERM_DAY -> $twenty $three, $twenty -> 'twenty', $four -> 'four', TERM_DAY -> $twenty $four, $twenty -> 'twenty', $five -> 'five', TERM_DAY -> $twenty $five, $twenty -> 'twenty', $six -> 'six', TERM_DAY -> $twenty $six, $twenty -> 'twenty', $seven -> 'seven', TERM_DAY -> $twenty $seven, $twenty -> 'twenty', $eight -> 'eight', TERM_DAY -> $twenty $eight, $twenty -> 'twenty', $nine -> 'nine', TERM_DAY -> $twenty $nine, TERM_DAY -> 'thirty', $thirty -> 'thirty', $one -> 'one', TERM_DAY -> $thirty $one, TERM_YEAR -> '1991', TERM_YEAR -> '1992', PP_TIME -> P_TIME NP_TIME, $that -> 'that', P_TIME -> $that P_TIME|<@arrive-@before>, $arrive -> 'arrive', $before -> 'before', P_TIME|<@arrive-@before> -> $arrive $before, $that -> 'that', P_TIME -> $that P_TIME|<@arrives-@before>, $arrives -> 'arrives', $before -> 'before', P_TIME|<@arrives-@before> -> $arrives $before, $arriving -> 'arriving', $before -> 'before', P_TIME -> $arriving $before, $arrival -> 'arrival', $by -> 'by', P_TIME -> $arrival $by, $which -> 'which', P_TIME -> $which P_TIME|<@arrive-@before>, $arrive -> 'arrive', $before -> 'before', P_TIME|<@arrive-@before> -> $arrive $before, $departing -> 'departing', $before -> 'before', P_TIME -> $departing $before, P_TIME -> 'before', $that -> 'that', P_TIME -> $that P_TIME|<@leaves-@before>, $leaves -> 'leaves', $before -> 'before', P_TIME|<@leaves-@before> -> $leaves $before, P_TIME -> 'by', $that -> 'that', P_TIME -> $that P_TIME|<@return-@around>, $return -> 'return', $around -> 'around', P_TIME|<@return-@around> -> $return $around, $that -> 'that', P_TIME -> $that P_TIME|<@gets-@in-@around>, $gets -> 'gets', P_TIME|<@gets-@in-@around> -> $gets P_TIME|<@in-@around>, $in -> 'in', $around -> 'around', P_TIME|<@in-@around> -> $in $around, $arriving -> 'arriving', $around -> 'around', P_TIME -> $arriving $around, $arriving -> 'arriving', $about -> 'about', P_TIME -> $arriving $about, $that -> 'that', P_TIME -> $that P_TIME|<@arrive-@soon-@after>, $arrive -> 'arrive', P_TIME|<@arrive-@soon-@after> -> $arrive P_TIME|<@soon-@after>, $soon -> 'soon', $after -> 'after', P_TIME|<@soon-@after> -> $soon $after, P_TIME -> 'around', P_TIME -> 'arrives', $arriving -> 'arriving', $at -> 'at', P_TIME -> $arriving $at, P_TIME -> 'arriving', $leaving -> 'leaving', $at -> 'at', P_TIME -> $leaving $at, P_TIME -> 'at', P_TIME -> 'leaving', P_TIME -> 'in', $departing -> 'departing', $at -> 'at', P_TIME -> $departing $at, P_TIME -> 'on', $that -> 'that', P_TIME -> $that P_TIME|<@leaves-@at>, $leaves -> 'leaves', $at -> 'at', P_TIME|<@leaves-@at> -> $leaves $at, $arriving -> 'arriving', $after -> 'after', P_TIME -> $arriving $after, $which -> 'which', P_TIME -> $which P_TIME|<@arrives-@after>, $arrives -> 'arrives', $after -> 'after', P_TIME|<@arrives-@after> -> $arrives $after, $that -> 'that', P_TIME -> $that P_TIME|<@arrives-@after>, $arrives -> 'arrives', $after -> 'after', P_TIME|<@arrives-@after> -> $arrives $after, $which -> 'which', P_TIME -> $which P_TIME|<@leave-@after>, $leave -> 'leave', $after -> 'after', P_TIME|<@leave-@after> -> $leave $after, $leaving -> 'leaving', $after -> 'after', P_TIME -> $leaving $after, P_TIME -> 'after', $departing -> 'departing', $after -> 'after', P_TIME -> $departing $after, $departing -> 'departing', $in -> 'in', P_TIME -> $departing $in, $that -> 'that', P_TIME -> $that P_TIME|<@depart-@after>, $depart -> 'depart', $after -> 'after', P_TIME|<@depart-@after> -> $depart $after, $that -> 'that', P_TIME -> $that P_TIME|<@leaves-@in>, $leaves -> 'leaves', $in -> 'in', P_TIME|<@leaves-@in> -> $leaves $in, NP_TIME -> 'afternoons', $the -> 'the', $afternoon -> 'afternoon', NP_TIME -> $the $afternoon, $the -> 'the', NP_TIME -> $the NP_TIME|<@late-@afternoon>, $late -> 'late', $afternoon -> 'afternoon', NP_TIME|<@late-@afternoon> -> $late $afternoon, NP_TIME -> 'evenings', $the -> 'the', $evening -> 'evening', NP_TIME -> $the $evening, NP_TIME -> 'mornings', $the -> 'the', $morning -> 'morning', NP_TIME -> $the $morning, $the -> 'the', NP_TIME -> $the NP_TIME|<@early-@am>, $early -> 'early', $am -> 'am', NP_TIME|<@early-@am> -> $early $am, $as -> 'as', NP_TIME -> $as NP_TIME|<@early-@as-@possible>, $early -> 'early', NP_TIME|<@early-@as-@possible> -> $early NP_TIME|<@as-@possible>, $as -> 'as', $possible -> 'possible', NP_TIME|<@as-@possible> -> $as $possible, $earliest -> 'earliest', NP_TIME -> $earliest NP_TIME|<@possible-@time>, $possible -> 'possible', $time -> 'time', NP_TIME|<@possible-@time> -> $possible $time, $the -> 'the', $day -> 'day', NP_TIME -> $the $day, $as -> 'as', NP_TIME -> $as NP_TIME|<@soon-@thereafter-@as-@possible>, $soon -> 'soon', NP_TIME|<@soon-@thereafter-@as-@possible> -> $soon NP_TIME|<@thereafter-@as-@possible>, $thereafter -> 'thereafter', NP_TIME|<@thereafter-@as-@possible> -> $thereafter NP_TIME|<@as-@possible>, $as -> 'as', $possible -> 'possible', NP_TIME|<@as-@possible> -> $as $possible, $lunch -> 'lunch', $time -> 'time', ADJ_TIME -> $lunch $time, ADJ_TIME -> 'evening', ADJ_TIME -> 'dinnertime', ADJ_TIME -> 'late', ADJ_TIME -> 'night', $latest -> 'latest', $possible -> 'possible', ADJ_TIME -> $latest $possible, ADJ_TIME -> 'latest', ADJ_TIME -> 'tonight', ADJ_TIME -> 'morning', ADJ_TIME -> 'early', $earliest -> 'earliest', $possible -> 'possible', ADJ_TIME -> $earliest $possible, ADJ_TIME -> 'earliest', ADJ_TIME -> 'afternoon', TERM_TIME -> 'one', TERM_TIME -> '1', TERM_TIME -> 'two', TERM_TIME -> '2', TERM_TIME -> 'three', TERM_TIME -> '3', TERM_TIME -> 'four', TERM_TIME -> '4', TERM_TIME -> 'five', TERM_TIME -> '5', TERM_TIME -> 'six', TERM_TIME -> '6', TERM_TIME -> 'seven', TERM_TIME -> '7', TERM_TIME -> 'eight', TERM_TIME -> '8', TERM_TIME -> 'nine', TERM_TIME -> '9', TERM_TIME -> 'ten', TERM_TIME -> '10', TERM_TIME -> 'eleven', TERM_TIME -> '11', TERM_TIME -> 'twelve', TERM_TIME -> '12', TERM_TIME -> 'noon', TERM_TIME -> 'midnight', TERM_TIME -> TERM_TIME TERM_TIMEMOD, TERM_TIMEMOD -> 'am', TERM_TIMEMOD -> 'oclock', TERM_TIMEMOD -> \"o'clock\", TERM_TIMEMOD -> 'noon', TERM_TIMEMOD -> 'pm', TERM_TIMEMOD -> 'midnight', ADJ_PLACE -> '...your grammar here...', PP_PLACE -> P_PLACE TERM_PLACE, PP_PLACE -> P_PLACE TERM_AIRPORT, $form -> 'form', PP_PLACE -> $form PP_PLACE|<TERM_PLACE-@to-TERM_PLACE>, PP_PLACE|<TERM_PLACE-@to-TERM_PLACE> -> TERM_PLACE PP_PLACE|<@to-TERM_PLACE>, $to -> 'to', PP_PLACE|<@to-TERM_PLACE> -> $to TERM_PLACE, $between -> 'between', PP_PLACE -> $between PP_PLACE|<TERM_PLACE-@and-TERM_PLACE>, PP_PLACE|<TERM_PLACE-@and-TERM_PLACE> -> TERM_PLACE PP_PLACE|<@and-TERM_PLACE>, $and -> 'and', PP_PLACE|<@and-TERM_PLACE> -> $and TERM_PLACE, $form -> 'form', PP_PLACE -> $form PP_PLACE|<TERM_AIRPORT-@to-TERM_AIRPORT>, PP_PLACE|<TERM_AIRPORT-@to-TERM_AIRPORT> -> TERM_AIRPORT PP_PLACE|<@to-TERM_AIRPORT>, $to -> 'to', PP_PLACE|<@to-TERM_AIRPORT> -> $to TERM_AIRPORT, $between -> 'between', PP_PLACE -> $between PP_PLACE|<TERM_AIRPORT-@and-TERM_AIRPORT>, PP_PLACE|<TERM_AIRPORT-@and-TERM_AIRPORT> -> TERM_AIRPORT PP_PLACE|<@and-TERM_AIRPORT>, $and -> 'and', PP_PLACE|<@and-TERM_AIRPORT> -> $and TERM_AIRPORT, P_PLACE -> 'to', $that -> 'that', P_PLACE -> $that P_PLACE|<@arrive-@at>, $arrive -> 'arrive', $at -> 'at', P_PLACE|<@arrive-@at> -> $arrive $at, $that -> 'that', P_PLACE -> $that P_PLACE|<@arrives-@in>, $arrives -> 'arrives', $in -> 'in', P_PLACE|<@arrives-@in> -> $arrives $in, $coming -> 'coming', P_PLACE -> $coming P_PLACE|<@back-@to>, $back -> 'back', $to -> 'to', P_PLACE|<@back-@to> -> $back $to, $that -> 'that', P_PLACE -> $that P_PLACE|<@go-@to>, $go -> 'go', $to -> 'to', P_PLACE|<@go-@to> -> $go $to, $and -> 'and', P_PLACE -> $and P_PLACE|<@then-@to>, $then -> 'then', $to -> 'to', P_PLACE|<@then-@to> -> $then $to, $arriving -> 'arriving', $in -> 'in', P_PLACE -> $arriving $in, $and -> 'and', P_PLACE -> $and P_PLACE|<@arriving-@in>, $arriving -> 'arriving', $in -> 'in', P_PLACE|<@arriving-@in> -> $arriving $in, $and -> 'and', P_PLACE -> $and P_PLACE|<@arrive-@in>, $arrive -> 'arrive', $in -> 'in', P_PLACE|<@arrive-@in> -> $arrive $in, $to -> 'to', P_PLACE -> $to P_PLACE|<@arrive-@in>, $arrive -> 'arrive', $in -> 'in', P_PLACE|<@arrive-@in> -> $arrive $in, $arrive -> 'arrive', $in -> 'in', P_PLACE -> $arrive $in, $going -> 'going', $to -> 'to', P_PLACE -> $going $to, P_PLACE -> 'into', P_PLACE -> 'for', $with -> 'with', P_PLACE -> $with P_PLACE|<@the-@destination-@city-@of>, $the -> 'the', P_PLACE|<@the-@destination-@city-@of> -> $the P_PLACE|<@destination-@city-@of>, $destination -> 'destination', P_PLACE|<@destination-@city-@of> -> $destination P_PLACE|<@city-@of>, $city -> 'city', $of -> 'of', P_PLACE|<@city-@of> -> $city $of, P_PLACE -> 'arriving', $goes -> 'goes', $to -> 'to', P_PLACE -> $goes $to, $flying -> 'flying', $into -> 'into', P_PLACE -> $flying $into, $goes -> 'goes', P_PLACE -> $goes P_PLACE|<@on-@to>, $on -> 'on', $to -> 'to', P_PLACE|<@on-@to> -> $on $to, P_PLACE -> 'reaching', P_PLACE -> 'in', $and -> 'and', $then -> 'then', P_PLACE -> $and $then, $arriving -> 'arriving', $to -> 'to', P_PLACE -> $arriving $to, P_PLACE -> 'from', P_PLACE -> 'leaving', $return -> 'return', $from -> 'from', P_PLACE -> $return $from, $leaving -> 'leaving', $from -> 'from', P_PLACE -> $leaving $from, $departing -> 'departing', $from -> 'from', P_PLACE -> $departing $from, P_PLACE -> 'departing', $go -> 'go', $from -> 'from', P_PLACE -> $go $from, $going -> 'going', $from -> 'from', P_PLACE -> $going $from, $back -> 'back', $from -> 'from', P_PLACE -> $back $from, $that -> 'that', P_PLACE -> $that P_PLACE|<@goes-@from>, $goes -> 'goes', $from -> 'from', P_PLACE|<@goes-@from> -> $goes $from, $that -> 'that', $departs -> 'departs', P_PLACE -> $that $departs, $which -> 'which', P_PLACE -> $which P_PLACE|<@leaves-@from>, $leaves -> 'leaves', $from -> 'from', P_PLACE|<@leaves-@from> -> $leaves $from, $which -> 'which', $leave -> 'leave', P_PLACE -> $which $leave, $that -> 'that', $leave -> 'leave', P_PLACE -> $that $leave, $originating -> 'originating', $in -> 'in', P_PLACE -> $originating $in, P_PLACE -> 'leave', $out -> 'out', $of -> 'of', P_PLACE -> $out $of, $leaves -> 'leaves', $from -> 'from', P_PLACE -> $leaves $from, $to -> 'to', P_PLACE -> $to P_PLACE|<@get-@from>, $get -> 'get', $from -> 'from', P_PLACE|<@get-@from> -> $get $from, P_PLACE -> 'via', $with -> 'with', P_PLACE -> $with P_PLACE|<@a-@stopover-@in>, $a -> 'a', P_PLACE|<@a-@stopover-@in> -> $a P_PLACE|<@stopover-@in>, $stopover -> 'stopover', $in -> 'in', P_PLACE|<@stopover-@in> -> $stopover $in, $with -> 'with', P_PLACE -> $with P_PLACE|<@a-@layover-@in>, $a -> 'a', P_PLACE|<@a-@layover-@in> -> $a P_PLACE|<@layover-@in>, $layover -> 'layover', $in -> 'in', P_PLACE|<@layover-@in> -> $layover $in, $with -> 'with', P_PLACE -> $with P_PLACE|<@a-@stopover-@at>, $a -> 'a', P_PLACE|<@a-@stopover-@at> -> $a P_PLACE|<@stopover-@at>, $stopover -> 'stopover', $at -> 'at', P_PLACE|<@stopover-@at> -> $stopover $at, $and -> 'and', P_PLACE -> $and P_PLACE|<@a-@stopover-@in>, $a -> 'a', P_PLACE|<@a-@stopover-@in> -> $a P_PLACE|<@stopover-@in>, $stopover -> 'stopover', $in -> 'in', P_PLACE|<@stopover-@in> -> $stopover $in, $stop -> 'stop', $in -> 'in', P_PLACE -> $stop $in, $stopping -> 'stopping', $in -> 'in', P_PLACE -> $stopping $in, $make -> 'make', P_PLACE -> $make P_PLACE|<@a-@stop-@in>, $a -> 'a', P_PLACE|<@a-@stop-@in> -> $a P_PLACE|<@stop-@in>, $stop -> 'stop', $in -> 'in', P_PLACE|<@stop-@in> -> $stop $in, $with -> 'with', P_PLACE -> $with P_PLACE|<@a-@stop-@in>, $a -> 'a', P_PLACE|<@a-@stop-@in> -> $a P_PLACE|<@stop-@in>, $stop -> 'stop', $in -> 'in', P_PLACE|<@stop-@in> -> $stop $in, $with -> 'with', P_PLACE -> $with P_PLACE|<@one-@stop-@in>, $one -> 'one', P_PLACE|<@one-@stop-@in> -> $one P_PLACE|<@stop-@in>, $stop -> 'stop', $in -> 'in', P_PLACE|<@stop-@in> -> $stop $in, $go -> 'go', $through -> 'through', P_PLACE -> $go $through, $which -> 'which', P_PLACE -> $which P_PLACE|<@go-@through>, $go -> 'go', $through -> 'through', P_PLACE|<@go-@through> -> $go $through, $makes -> 'makes', P_PLACE -> $makes P_PLACE|<@a-@stopover-@in>, $a -> 'a', P_PLACE|<@a-@stopover-@in> -> $a P_PLACE|<@stopover-@in>, $stopover -> 'stopover', $in -> 'in', P_PLACE|<@stopover-@in> -> $stopover $in, $that -> 'that', P_PLACE -> $that P_PLACE|<@stops-@in>, $stops -> 'stops', $in -> 'in', P_PLACE|<@stops-@in> -> $stops $in, $that -> 'that', P_PLACE -> $that P_PLACE|<@stops-@over-@in>, $stops -> 'stops', P_PLACE|<@stops-@over-@in> -> $stops P_PLACE|<@over-@in>, $over -> 'over', $in -> 'in', P_PLACE|<@over-@in> -> $over $in, $by -> 'by', P_PLACE -> $by P_PLACE|<@way-@of>, $way -> 'way', $of -> 'of', P_PLACE|<@way-@of> -> $way $of, $connecting -> 'connecting', $through -> 'through', P_PLACE -> $connecting $through, $that -> 'that', P_PLACE -> $that P_PLACE|<@will-@stop-@in>, $will -> 'will', P_PLACE|<@will-@stop-@in> -> $will P_PLACE|<@stop-@in>, $stop -> 'stop', $in -> 'in', P_PLACE|<@stop-@in> -> $stop $in, $which -> 'which', P_PLACE -> $which P_PLACE|<@connects-@in>, $connects -> 'connects', $in -> 'in', P_PLACE|<@connects-@in> -> $connects $in, $arriving -> 'arriving', P_PLACE -> $arriving P_PLACE|<@and-@departing-@at>, $and -> 'and', P_PLACE|<@and-@departing-@at> -> $and P_PLACE|<@departing-@at>, $departing -> 'departing', $at -> 'at', P_PLACE|<@departing-@at> -> $departing $at, $boston -> 'boston', $logan -> 'logan', TERM_AIRPORT -> $boston $logan, TERM_AIRPORT -> 'logan', TERM_AIRPORT -> 'bwi', $general -> 'general', TERM_AIRPORT -> $general TERM_AIRPORT|<@mitchell-@international>, $mitchell -> 'mitchell', $international -> 'international', TERM_AIRPORT|<@mitchell-@international> -> $mitchell $international, $general -> 'general', $mitchell -> 'mitchell', TERM_AIRPORT -> $general $mitchell, TERM_AIRPORT -> 'jfk', $kennedy -> 'kennedy', $airport -> 'airport', TERM_AIRPORT -> $kennedy $airport, $laguardia -> 'laguardia', $airport -> 'airport', TERM_AIRPORT -> $laguardia $airport, $love -> 'love', $field -> 'field', TERM_AIRPORT -> $love $field, TERM_PLACE -> 'anywhere', TERM_PLACE -> 'atlanta', TERM_PLACE -> 'austin', TERM_PLACE -> 'baltimore', TERM_PLACE -> 'boston', TERM_PLACE -> 'burbank', TERM_PLACE -> 'charlotte', TERM_PLACE -> 'chicago', TERM_PLACE -> 'cincinnati', TERM_PLACE -> 'cleveland', $cleveland -> 'cleveland', $ohio -> 'ohio', TERM_PLACE -> $cleveland $ohio, TERM_PLACE -> 'columbus', $dallas -> 'dallas', TERM_PLACE -> $dallas TERM_PLACE|<@fort-@worth>, $fort -> 'fort', $worth -> 'worth', TERM_PLACE|<@fort-@worth> -> $fort $worth, TERM_PLACE -> 'dallas', TERM_PLACE -> 'denver', $denver -> 'denver', $colorado -> 'colorado', TERM_PLACE -> $denver $colorado, TERM_PLACE -> 'detroit', $fort -> 'fort', $worth -> 'worth', TERM_PLACE -> $fort $worth, TERM_PLACE -> 'houston', TERM_PLACE -> 'indianapolis', $kansas -> 'kansas', $city -> 'city', TERM_PLACE -> $kansas $city, $las -> 'las', $vegas -> 'vegas', TERM_PLACE -> $las $vegas, $long -> 'long', $beach -> 'beach', TERM_PLACE -> $long $beach, $los -> 'los', $angeles -> 'angeles', TERM_PLACE -> $los $angeles, TERM_PLACE -> 'memphis', TERM_PLACE -> 'miami', TERM_PLACE -> 'milwaukee', TERM_PLACE -> 'minneapolis', TERM_PLACE -> 'montreal', $montreal -> 'montreal', $quebec -> 'quebec', TERM_PLACE -> $montreal $quebec, $montreal -> 'montreal', $canada -> 'canada', TERM_PLACE -> $montreal $canada, TERM_PLACE -> 'nashville', $new -> 'new', TERM_PLACE -> $new TERM_PLACE|<@york-@city>, $york -> 'york', $city -> 'city', TERM_PLACE|<@york-@city> -> $york $city, $new -> 'new', $york -> 'york', TERM_PLACE -> $new $york, TERM_PLACE -> 'newark', $newark -> 'newark', TERM_PLACE -> $newark TERM_PLACE|<@new-@jersey>, $new -> 'new', $jersey -> 'jersey', TERM_PLACE|<@new-@jersey> -> $new $jersey, TERM_PLACE -> 'oakland', $oakland -> 'oakland', $california -> 'california', TERM_PLACE -> $oakland $california, TERM_PLACE -> 'ontario', TERM_PLACE -> 'orlando', $orlando -> 'orlando', $florida -> 'florida', TERM_PLACE -> $orlando $florida, TERM_PLACE -> 'philadelphia', TERM_PLACE -> 'philly', TERM_PLACE -> 'phoenix', TERM_PLACE -> 'pittsburgh', $salt -> 'salt', TERM_PLACE -> $salt TERM_PLACE|<@lake-@city>, $lake -> 'lake', $city -> 'city', TERM_PLACE|<@lake-@city> -> $lake $city, $san -> 'san', $diego -> 'diego', TERM_PLACE -> $san $diego, $san -> 'san', TERM_PLACE -> $san TERM_PLACE|<@diego-@california>, $diego -> 'diego', $california -> 'california', TERM_PLACE|<@diego-@california> -> $diego $california, $san -> 'san', $francisco -> 'francisco', TERM_PLACE -> $san $francisco, $san -> 'san', $jose -> 'jose', TERM_PLACE -> $san $jose, TERM_PLACE -> 'seattle', $st. -> 'st.', $louis -> 'louis', TERM_PLACE -> $st. $louis, $st. -> 'st.', $paul -> 'paul', TERM_PLACE -> $st. $paul, $st. -> 'st.', $petersburg -> 'petersburg', TERM_PLACE -> $st. $petersburg, TERM_PLACE -> 'tacoma', $tacoma -> 'tacoma', $washington -> 'washington', TERM_PLACE -> $tacoma $washington, TERM_PLACE -> 'tampa', TERM_PLACE -> 'toronto', TERM_PLACE -> 'washington', $washington -> 'washington', $dc -> 'dc', TERM_PLACE -> $washington $dc, TERM_PLACE -> 'dc', $westchester -> 'westchester', $county -> 'county', TERM_PLACE -> $westchester $county, $various -> 'various', $cities -> 'cities', TERM_PLACE -> $various $cities, PREIGNORE -> PREIGNORESYMBOL PREIGNORE, PREIGNORESYMBOL -> 'me', PREIGNORESYMBOL -> 'show', PREIGNORESYMBOL -> 'now', PREIGNORESYMBOL -> 'only', PREIGNORESYMBOL -> 'can', PREIGNORESYMBOL -> 'you', PREIGNORESYMBOL -> 'the', PREIGNORESYMBOL -> 'itinerary', PREIGNORESYMBOL -> 'of', PREIGNORESYMBOL -> 'also', PREIGNORESYMBOL -> 'a', PREIGNORESYMBOL -> 'list', PREIGNORESYMBOL -> 'could', PREIGNORESYMBOL -> 'give', PREIGNORESYMBOL -> 'which', PREIGNORESYMBOL -> 'what', PREIGNORESYMBOL -> 'is', PREIGNORESYMBOL -> \"what's\", PREIGNORESYMBOL -> 'are', PREIGNORESYMBOL -> 'my', PREIGNORESYMBOL -> 'choices', PREIGNORESYMBOL -> 'for', PREIGNORESYMBOL -> 'i', PREIGNORESYMBOL -> 'would', PREIGNORESYMBOL -> 'like', PREIGNORESYMBOL -> \"i'd\", PREIGNORESYMBOL -> 'to', PREIGNORESYMBOL -> 'see', PREIGNORESYMBOL -> 'have', PREIGNORESYMBOL -> 'make', PREIGNORESYMBOL -> 'book', PREIGNORESYMBOL -> 'find', PREIGNORESYMBOL -> 'information', PREIGNORESYMBOL -> 'on', PREIGNORESYMBOL -> 'know', PREIGNORESYMBOL -> 'some', PREIGNORESYMBOL -> 'hello', PREIGNORESYMBOL -> 'yes', PREIGNORESYMBOL -> 'please', PREIGNORESYMBOL -> 'repeat', PREIGNORESYMBOL -> 'do', PREIGNORESYMBOL -> 'have', PREIGNORESYMBOL -> 'there', PREIGNORESYMBOL -> 'need', PREIGNORESYMBOL -> 'hi', PREIGNORESYMBOL -> 'get', PREIGNORESYMBOL -> 'may', PREIGNORESYMBOL -> 'listing', PREIGNORESYMBOL -> 'listings', PREIGNORESYMBOL -> 'travel', PREIGNORESYMBOL -> 'arrangements', PREIGNORESYMBOL -> 'okay', PREIGNORESYMBOL -> 'want', PREIGNORESYMBOL -> 'tell', PREIGNORESYMBOL -> 'about', PREIGNORESYMBOL -> 'how', PREIGNORESYMBOL -> 'would', PREIGNORESYMBOL -> 'be', PREIGNORESYMBOL -> 'able', PREIGNORESYMBOL -> 'put', PREIGNORESYMBOL -> 'requesting', PREIGNORESYMBOL -> \"i'm\", PREIGNORESYMBOL -> 'looking', PREIGNORESYMBOL -> 'display', POSTIGNORE -> POSTIGNORESYMBOL POSTIGNORE, POSTIGNORESYMBOL -> 'please', POSTIGNORESYMBOL -> 'there', POSTIGNORESYMBOL -> 'are', POSTIGNORESYMBOL -> 'currently', POSTIGNORESYMBOL -> 'do', POSTIGNORESYMBOL -> 'you', POSTIGNORESYMBOL -> 'have', POSTIGNORESYMBOL -> 'fares', POSTIGNORESYMBOL -> 'information', POSTIGNORESYMBOL -> 'i', POSTIGNORESYMBOL -> 'want', POSTIGNORESYMBOL -> 'would', POSTIGNORESYMBOL -> 'like', POSTIGNORESYMBOL -> 'the', POSTIGNORESYMBOL -> 'flight', POSTIGNORESYMBOL -> 'be', POSTIGNORESYMBOL -> 'go', POSTIGNORESYMBOL -> 'departures', POSTIGNORESYMBOL -> 'is', POSTIGNORESYMBOL -> 'such', POSTIGNORESYMBOL -> 'a', POSTIGNORESYMBOL -> 'that', POSTIGNORESYMBOL -> 'serves', POSTIGNORESYMBOL -> 'both', POSTIGNORESYMBOL -> 'and', POSTIGNORESYMBOL -> 'along', POSTIGNORESYMBOL -> 'with', POSTIGNORESYMBOL -> 'can', POSTIGNORESYMBOL -> 'get', POSTIGNORESYMBOL -> \"i'd\", POSTIGNORESYMBOL -> 'traveling', POSTIGNORESYMBOL -> 'for', POSTIGNORESYMBOL -> 'me', POSTIGNORESYMBOL -> '.', POSTIGNORESYMBOL -> '?', S -> DET NOM_FLIGHT, NP_FLIGHT -> ADJ NOM_FLIGHT, NOM_FLIGHT -> N_FLIGHT PP, N_FLIGHT -> 'flights', N_FLIGHT -> 'flight', N_FLIGHT -> $to $fly, ADJ -> ADJ_FLIGHTTYPESIMPLE ADJ_FLIGHTTYPE|<@and-ADJ_FLIGHTTYPE>, ADJ -> ADJ_FLIGHTTYPESIMPLE ADJ_FLIGHTTYPE|<@or-ADJ_FLIGHTTYPE>, ADJ -> $lunch $time, ADJ -> 'evening', ADJ -> 'dinnertime', ADJ -> 'late', ADJ -> 'night', ADJ -> $latest $possible, ADJ -> 'latest', ADJ -> 'tonight', ADJ -> 'morning', ADJ -> 'early', ADJ -> $earliest $possible, ADJ -> 'earliest', ADJ -> 'afternoon', ADJ -> 'cheapest', ADJ -> $lowest $cost, ADJ -> $least $expensive, ADJ -> 'inexpensive', ADJ -> 'cheap', ADJ -> 'expensive', ADJ -> $highest $cost, ADJ -> $most $expensive, ADJ -> 'economy', ADJ -> $thrift $economy, ADJ -> $first $class, ADJ -> 'transcontinental', ADJ -> 'available', ADJ -> 'possible', ADJ -> '...your grammar here...', PP -> $on PP_AIRLINE|<TERM_AIRBRAND-TERM_AIRBRANDTYPE>, PP -> $on TERM_AIRBRAND, PP -> P_DATE NP_DATE, PP -> P_TIME NP_TIME, PP -> P_PLACE TERM_PLACE, PP -> P_PLACE TERM_AIRPORT, PP -> $form PP_PLACE|<TERM_PLACE-@to-TERM_PLACE>, PP -> $between PP_PLACE|<TERM_PLACE-@and-TERM_PLACE>, PP -> $form PP_PLACE|<TERM_AIRPORT-@to-TERM_AIRPORT>, PP -> $between PP_PLACE|<TERM_AIRPORT-@and-TERM_AIRPORT>, PP -> $less PP_PRICE|<@than-AMOUNT-@dollars>, PP -> $with PP_PRICE|<@the-@lowest-@fare>, PP -> $the PP_PRICE|<@cheapest-@way-@possible>, PP -> $with PP_PRICE|<@the-@highest-@fare>, PP -> P_FOOD NP_FOOD, ADJ_AIRLINE -> 'continental', ADJ_AIRLINE -> 'american', ADJ_AIRLINE -> 'united', ADJ_AIRLINE -> 'northwest', ADJ_AIRLINE -> 'us', ADJ_AIRLINE -> 'delta', ADJ_AIRLINE -> 'twa', ADJ_AIRLINE -> $air $canada, ADJ_AIRLINE -> $midwest $express, ADJ_AIRLINE -> $trans $world, ADJ_FLIGHTTYPE -> $round $trip, ADJ_FLIGHTTYPE -> 'round-trip', ADJ_FLIGHTTYPE -> 'return', ADJ_FLIGHTTYPE -> $one $way, ADJ_FLIGHTTYPE -> 'nonstop', ADJ_FLIGHTTYPE -> 'direct', ADJ_FLIGHTTYPE -> 'connecting', PP_CLASS -> 'economy', PP_CLASS -> $thrift $economy, PP_CLASS -> $first $class, PP_CLASS -> 'transcontinental', PP_CLASS -> 'available', PP_CLASS -> 'possible', ADJ_FOOD -> 'dinner', ADJ_FOOD -> 'lunch', ADJ_FOOD -> 'breakfast', ADJ_FOOD -> $a $meal, ADJ_DATE -> 'saturday', ADJ_DATE -> 'sunday', ADJ_DATE -> 'monday', ADJ_DATE -> 'tuesday', ADJ_DATE -> 'wednesday', ADJ_DATE -> 'thursday', ADJ_DATE -> 'friday', ADJ_DATE -> 'weekday', PP_DATE -> $a TERM_WEEKDAY, PP_DATE -> $this TERM_WEEKDAY, PP_DATE -> $this $coming, PP_DATE -> TERM_WEEKDAY ADJ_TIME, PP_DATE -> 'saturdays', PP_DATE -> 'sundays', PP_DATE -> 'mondays', PP_DATE -> 'tuesdays', PP_DATE -> 'wednesdays', PP_DATE -> 'thursdays', PP_DATE -> 'fridays', PP_DATE -> 'day', PP_DATE -> 'week', PP_DATE -> 'today', PP_DATE -> 'tomorrow', PP_DATE -> $the NP_DATE|<@day-@after-@tomorrow>, PP_DATE -> 'weekdays', NP_DATE -> 'saturday', NP_DATE -> 'sunday', NP_DATE -> 'monday', NP_DATE -> 'tuesday', NP_DATE -> 'wednesday', NP_DATE -> 'thursday', NP_DATE -> 'friday', NP_DATE -> 'weekday', NP_DATE -> TERM_MONTH NP_MDYDATE|<TERM_DAY-YEAR>, NP_DATE -> TERM_MONTH TERM_DAY, NP_DATE -> $the TERM_DAY, NP_DATE -> TERM_MONTH NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, NP_DATE -> $the NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, NP_DATE -> $either NP_MDYDATE|<TERM_MONTH-TERM_DAY-@or-TERM_DAY>, NP_DATE -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY>, NP_DATE -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH>, NP_DATE -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, NP_DATE -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH>, NP_DATE -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH>, NP_DATE -> $either NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, NP_TIME -> 'one', NP_TIME -> '1', NP_TIME -> 'two', NP_TIME -> '2', NP_TIME -> 'three', NP_TIME -> '3', NP_TIME -> 'four', NP_TIME -> '4', NP_TIME -> 'five', NP_TIME -> '5', NP_TIME -> 'six', NP_TIME -> '6', NP_TIME -> 'seven', NP_TIME -> '7', NP_TIME -> 'eight', NP_TIME -> '8', NP_TIME -> 'nine', NP_TIME -> '9', NP_TIME -> 'ten', NP_TIME -> '10', NP_TIME -> 'eleven', NP_TIME -> '11', NP_TIME -> 'twelve', NP_TIME -> '12', NP_TIME -> 'noon', NP_TIME -> 'midnight', NP_TIME -> TERM_TIME TERM_TIMEMOD, PREIGNORE -> 'me', PREIGNORE -> 'show', PREIGNORE -> 'now', PREIGNORE -> 'only', PREIGNORE -> 'can', PREIGNORE -> 'you', PREIGNORE -> 'the', PREIGNORE -> 'itinerary', PREIGNORE -> 'of', PREIGNORE -> 'also', PREIGNORE -> 'a', PREIGNORE -> 'list', PREIGNORE -> 'could', PREIGNORE -> 'give', PREIGNORE -> 'which', PREIGNORE -> 'what', PREIGNORE -> 'is', PREIGNORE -> \"what's\", PREIGNORE -> 'are', PREIGNORE -> 'my', PREIGNORE -> 'choices', PREIGNORE -> 'for', PREIGNORE -> 'i', PREIGNORE -> 'would', PREIGNORE -> 'like', PREIGNORE -> \"i'd\", PREIGNORE -> 'to', PREIGNORE -> 'see', PREIGNORE -> 'have', PREIGNORE -> 'make', PREIGNORE -> 'book', PREIGNORE -> 'find', PREIGNORE -> 'information', PREIGNORE -> 'on', PREIGNORE -> 'know', PREIGNORE -> 'some', PREIGNORE -> 'hello', PREIGNORE -> 'yes', PREIGNORE -> 'please', PREIGNORE -> 'repeat', PREIGNORE -> 'do', PREIGNORE -> 'have', PREIGNORE -> 'there', PREIGNORE -> 'need', PREIGNORE -> 'hi', PREIGNORE -> 'get', PREIGNORE -> 'may', PREIGNORE -> 'listing', PREIGNORE -> 'listings', PREIGNORE -> 'travel', PREIGNORE -> 'arrangements', PREIGNORE -> 'okay', PREIGNORE -> 'want', PREIGNORE -> 'tell', PREIGNORE -> 'about', PREIGNORE -> 'how', PREIGNORE -> 'would', PREIGNORE -> 'be', PREIGNORE -> 'able', PREIGNORE -> 'put', PREIGNORE -> 'requesting', PREIGNORE -> \"i'm\", PREIGNORE -> 'looking', PREIGNORE -> 'display', POSTIGNORE -> 'please', POSTIGNORE -> 'there', POSTIGNORE -> 'are', POSTIGNORE -> 'currently', POSTIGNORE -> 'do', POSTIGNORE -> 'you', POSTIGNORE -> 'have', POSTIGNORE -> 'fares', POSTIGNORE -> 'information', POSTIGNORE -> 'i', POSTIGNORE -> 'want', POSTIGNORE -> 'would', POSTIGNORE -> 'like', POSTIGNORE -> 'the', POSTIGNORE -> 'flight', POSTIGNORE -> 'be', POSTIGNORE -> 'go', POSTIGNORE -> 'departures', POSTIGNORE -> 'is', POSTIGNORE -> 'such', POSTIGNORE -> 'a', POSTIGNORE -> 'that', POSTIGNORE -> 'serves', POSTIGNORE -> 'both', POSTIGNORE -> 'and', POSTIGNORE -> 'along', POSTIGNORE -> 'with', POSTIGNORE -> 'can', POSTIGNORE -> 'get', POSTIGNORE -> \"i'd\", POSTIGNORE -> 'traveling', POSTIGNORE -> 'for', POSTIGNORE -> 'me', POSTIGNORE -> '.', POSTIGNORE -> '?', S -> ADJ NOM_FLIGHT, NP_FLIGHT -> N_FLIGHT PP, NOM_FLIGHT -> 'flights', NOM_FLIGHT -> 'flight', NOM_FLIGHT -> $to $fly, ADJ -> 'continental', ADJ -> 'american', ADJ -> 'united', ADJ -> 'northwest', ADJ -> 'us', ADJ -> 'delta', ADJ -> 'twa', ADJ -> $air $canada, ADJ -> $midwest $express, ADJ -> $trans $world, ADJ -> 'saturday', ADJ -> 'sunday', ADJ -> 'monday', ADJ -> 'tuesday', ADJ -> 'wednesday', ADJ -> 'thursday', ADJ -> 'friday', ADJ -> 'weekday', ADJ -> $round $trip, ADJ -> 'round-trip', ADJ -> 'return', ADJ -> $one $way, ADJ -> 'nonstop', ADJ -> 'direct', ADJ -> 'connecting', ADJ -> 'dinner', ADJ -> 'lunch', ADJ -> 'breakfast', ADJ -> $a $meal, PP -> $a TERM_WEEKDAY, PP -> $this TERM_WEEKDAY, PP -> $this $coming, PP -> TERM_WEEKDAY ADJ_TIME, PP -> 'saturdays', PP -> 'sundays', PP -> 'mondays', PP -> 'tuesdays', PP -> 'wednesdays', PP -> 'thursdays', PP -> 'fridays', PP -> 'day', PP -> 'week', PP -> 'today', PP -> 'tomorrow', PP -> $the NP_DATE|<@day-@after-@tomorrow>, PP -> 'weekdays', PP -> 'economy', PP -> $thrift $economy, PP -> $first $class, PP -> 'transcontinental', PP -> 'available', PP -> 'possible', PP_DATE -> 'saturday', PP_DATE -> 'sunday', PP_DATE -> 'monday', PP_DATE -> 'tuesday', PP_DATE -> 'wednesday', PP_DATE -> 'thursday', PP_DATE -> 'friday', PP_DATE -> 'weekday', PP_DATE -> TERM_MONTH NP_MDYDATE|<TERM_DAY-YEAR>, PP_DATE -> TERM_MONTH TERM_DAY, PP_DATE -> $the TERM_DAY, PP_DATE -> TERM_MONTH NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, PP_DATE -> $the NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, PP_DATE -> $either NP_MDYDATE|<TERM_MONTH-TERM_DAY-@or-TERM_DAY>, PP_DATE -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY>, PP_DATE -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH>, PP_DATE -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, PP_DATE -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH>, PP_DATE -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH>, PP_DATE -> $either NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, S -> N_FLIGHT PP, NP_FLIGHT -> 'flights', NP_FLIGHT -> 'flight', NP_FLIGHT -> $to $fly, PP -> 'saturday', PP -> 'sunday', PP -> 'monday', PP -> 'tuesday', PP -> 'wednesday', PP -> 'thursday', PP -> 'friday', PP -> 'weekday', PP -> TERM_MONTH NP_MDYDATE|<TERM_DAY-YEAR>, PP -> TERM_MONTH TERM_DAY, PP -> $the TERM_DAY, PP -> TERM_MONTH NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, PP -> $the NP_MDYDATE|<TERM_DAY-@or-TERM_DAY>, PP -> $either NP_MDYDATE|<TERM_MONTH-TERM_DAY-@or-TERM_DAY>, PP -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY>, PP -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH>, PP -> TERM_DAY NP_MDYDATE|<@of-TERM_MONTH>, PP -> $either NP_MDYDATE|<@the-TERM_DAY-@or-@the-TERM_DAY-@of-TERM_MONTH>, PP -> $the NP_MDYDATE|<TERM_DAY-@of-TERM_MONTH-@or-@the-@day-@of-TERM_MONTH>, PP -> $either NP_MDYDATE|<@the-TERM_DAY-@of-TERM_MONTH-@or-@the-TERM_DAY-@of-TERM_MONTH>, S -> 'flights', S -> 'flight', S -> $to $fly]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-62425f5c0537>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matis_grammar_cnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproductions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mrecognized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"+\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcky_recognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matis_grammar_cnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{recognized:5}{sentence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-176-bd79030ca0fe>\u001b[0m in \u001b[0;36mcky_recognize\u001b[0;34m(grammar, tokens)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mtable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munited_not_airlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#print(table_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtable_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-176-bd79030ca0fe>\u001b[0m in \u001b[0;36munited_not_airlines\u001b[0;34m(grammar, B, C)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#print(rule.rhs()[0],rule.rhs()[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#print(b,c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0munited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not Nonterminal"
          ]
        }
      ],
      "source": [
        "test_sentences = [\"show me flights from boston\",\n",
        "                  \"show me united flights before noon\",\n",
        "                  \"are there any twa flights available tomorrow\",\n",
        "                  \"show me flights united are there any\"]\n",
        "print(atis_grammar_cnf.productions())\n",
        "for sentence in test_sentences:\n",
        "  recognized = \"+\" if cky_recognize(atis_grammar_cnf, tokenize(sentence)) else \"-\"\n",
        "  print(f\"{recognized:5}{sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a891d62b",
      "metadata": {
        "id": "a891d62b"
      },
      "source": [
        "You can also verify that the CKY recognizer verifies the same coverage as the NLTK parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "7699ae84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7699ae84",
        "outputId": "c80435a6-38bc-4936-8d11-82ff24fe10e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 65886.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed: ['list', 'all', 'the', 'flights', 'that', 'arrive', 'at', 'general', 'mitchell', 'international', 'from', 'various', 'cities']\n",
            "failed: ['give', 'me', 'the', 'flights', 'leaving', 'denver', 'august', 'ninth', 'coming', 'back', 'to', 'boston']\n",
            "failed: ['what', 'flights', 'from', 'tacoma', 'to', 'orlando', 'on', 'saturday']\n",
            "failed: ['what', 'is', 'the', 'most', 'expensive', 'one', 'way', 'fare', 'from', 'boston', 'to', 'atlanta', 'on', 'american', 'airlines']\n",
            "failed: ['what', 'flights', 'return', 'from', 'denver', 'to', 'philadelphia', 'on', 'a', 'saturday']\n",
            "failed: ['can', 'you', 'list', 'all', 'flights', 'from', 'chicago', 'to', 'milwaukee']\n",
            "failed: ['show', 'me', 'the', 'flights', 'from', 'denver', 'that', 'go', 'to', 'pittsburgh', 'and', 'then', 'atlanta']\n",
            "failed: ['i', \"'d\", 'like', 'to', 'see', 'flights', 'from', 'baltimore', 'to', 'atlanta', 'that', 'arrive', 'before', 'noon', 'and', 'i', \"'d\", 'like', 'to', 'see', 'flights', 'from', 'denver', 'to', 'atlanta', 'that', 'arrive', 'before', 'noon']\n",
            "failed: ['do', 'you', 'have', 'an', '819', 'flight', 'from', 'denver', 'to', 'san', 'francisco']\n",
            "failed: ['can', 'you', 'list', 'all', 'round', 'trip', 'flights', 'from', 'orlando', 'to', 'kansas', 'city', 'and', 'then', 'to', 'minneapolis']\n",
            "failed: ['show', 'me', 'the', 'united', 'flights', 'from', 'denver', 'to', 'baltimore']\n",
            "failed: ['i', 'would', 'like', 'a', 'us', 'air', 'flight', 'from', 'toronto', 'to', 'san', 'diego', 'with', 'a', 'stopover', 'in', 'denver', 'please']\n",
            "failed: ['are', 'there', 'any', 'nonstop', 'flights', 'leaving', 'from', 'denver', 'arriving', 'in', 'baltimore', 'on', 'july', 'seventh']\n",
            "failed: ['how', 'can', 'i', 'get', 'from', 'boston', 'to', 'atlanta', 'and', 'back', 'in', 'the', 'same', 'day', 'and', 'have', 'the', 'most', 'hours', 'on', 'the', 'ground', 'in', 'atlanta']\n",
            "failed: ['are', 'there', 'any', 'flights', 'between', 'pittsburgh', 'and', 'baltimore', 'using', 'a', 'j31', 'aircraft']\n",
            "failed: ['what', 'does', 'd', '/s', 'stand', 'for', 'for', 'meals']\n",
            "failed: ['pittsburgh', 'to', 'boston', 'saturday']\n",
            "failed: ['what', 'are', 'the', 'flights', 'from', 'charlotte', 'to', 'atlanta', 'returning', 'on', 'tuesday', 'july', 'thirteenth']\n",
            "failed: ['i', 'need', 'the', 'cost', 'of', 'a', 'ticket', 'going', 'from', 'denver', 'to', 'baltimore', 'a', 'first', 'class', 'ticket', 'on', 'united', 'airlines']\n",
            "failed: ['is', 'there', 'a', 'flight', 'between', 'san', 'francisco', 'and', 'boston', 'with', 'a', 'stopover', 'at', 'dallas', 'fort', 'worth']\n",
            "failed: ['list', 'the', 'takeoffs', 'and', 'landings', 'at', 'general', 'mitchell', 'international']\n",
            "failed: ['what', 'flights', 'are', 'there', 'from', 'minneapolis', 'to', 'newark', 'on', 'continental']\n",
            "failed: ['is', 'there', 'limo', 'service', 'at', 'pittsburgh', 'airport']\n",
            "failed: ['i', 'need', 'a', 'flight', 'from', 'pittsburgh', 'to', 'los', 'angeles', 'thursday', 'evening']\n",
            "failed: ['how', 'much', 'is', 'a', 'first', 'class', 'ticket', 'from', 'boston', 'to', 'san', 'francisco']\n",
            "failed: ['in', 'dallas', 'fort', 'worth', 'i', 'would', 'like', 'information', 'on', 'ground', 'transportation']\n",
            "failed: ['what', 'flights', 'go', 'from', 'philadelphia', 'to', 'san', 'francisco', 'via', 'dallas']\n",
            "failed: ['i', \"'d\", 'like', 'flights', 'on', 'american', 'airlines', 'from', 'philadelphia', 'philadelphia', 'to', 'dallas', 'arriving', 'before', '1145', 'am']\n",
            "failed: ['what', 'airlines', 'from', 'washington', 'dc', 'to', 'columbus']\n",
            "failed: ['what', 'flights', 'do', 'you', 'have', 'in', 'the', 'morning', 'of', 'september', 'twentieth', 'on', 'united', 'airlines', 'from', 'pittsburgh', 'to', 'san', 'francisco', 'and', 'a', 'stopover', 'in', 'denver']\n",
            "failed: ['what', 'is', 'the', 'cost', 'of', 'a', 'round', 'trip', 'flight', 'from', 'pittsburgh', 'to', 'atlanta', 'beginning', 'on', 'april', 'twenty', 'fifth', 'and', 'returning', 'on', 'may', 'sixth']\n",
            "failed: ['i', 'would', 'like', 'a', 'flight', 'from', 'washington', 'to', 'boston', 'leaving', 'at', '230', 'on', 'august', 'twentieth']\n",
            "failed: ['are', 'there', 'any', 'nonstop', 'flights', 'from', 'philadelphia', 'to', 'denver', 'that', 'arrive', 'before', '5', 'pm']\n",
            "failed: ['i', 'need', 'a', 'flight', 'from', 'baltimore', 'to', 'seattle']\n",
            "failed: ['i', \"'d\", 'like', 'to', 'fly', 'nonstop', 'from', 'atlanta', 'to', 'baltimore', 'and', 'get', 'there', 'at', '7', 'pm']\n",
            "failed: ['give', 'me', 'sunday', 'nonstop', 'flights', 'from', 'memphis', 'to', 'las', 'vegas']\n",
            "failed: ['what', 'is', 'the', 'type', 'of', 'aircraft', 'for', 'united', 'flight', '21']\n",
            "failed: ['can', 'you', 'give', 'me', 'information', 'on', 'transportation', 'from', 'the', 'airport', 'in', 'philadelphia', 'to', 'downtown', 'philadelphia']\n",
            "failed: ['list', 'flights', 'from', 'phoenix', 'to', 'detroit', 'on', 'wednesday']\n",
            "failed: ['what', 'is', 'the', 'fare', 'going', 'from', 'baltimore', 'to', 'boston', 'one', 'way', 'on', 'november', 'seventh']\n",
            "failed: ['i', 'would', 'like', 'to', 'see', 'the', 'flights', 'from', 'denver', 'to', 'philadelphia']\n",
            "failed: ['on', 'usa', 'air', 'how', 'many', 'flights', 'leaving', 'oakland', 'on', 'july', 'twenty', 'seventh', 'to', 'boston', 'nonstop']\n",
            "failed: ['what', 'is', 'the', 'earliest', 'flight', 'from', 'boston', 'to', 'atlanta']\n",
            "failed: ['all', 'flights', 'from', 'denver', 'to', 'philadelphia']\n",
            "failed: ['flights', 'from', 'cleveland', 'to', 'kansas', 'city']\n",
            "failed: ['show', 'me', 'the', 'first', 'class', 'fares', 'from', 'baltimore', 'to', 'dallas']\n",
            "failed: ['find', 'travel', 'arrangements', 'for', 'a', 'round', 'trip', 'flight', 'from', 'baltimore', 'to', 'pittsburgh']\n",
            "failed: ['show', 'me', 'flights', 'from', 'pittsburgh', 'to', 'san', 'francisco', 'on', 'sunday']\n",
            "failed: ['show', 'me', 'flights', 'between', 'new', 'york', 'city', 'and', 'las', 'vegas', 'on', 'sunday']\n",
            "failed: ['what', 'flights', 'do', 'you', 'have', 'from', 'burbank', 'to', 'tacoma', 'washington']\n",
            "0 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "coverage(lambda sent: cky_recognize(atis_grammar_cnf, sent),\n",
        "         training_corpus, n=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "368edfb3",
      "metadata": {
        "id": "368edfb3"
      },
      "source": [
        "## Part 3: Implement a CKY parser\n",
        "\n",
        "In part 2, you implemented a context-free grammar recognizer. Next, you'll implement a _parser_.\n",
        "\n",
        "Implement the CKY algorithm for parsing with CFGs as a function `cky_parse`, which takes a grammar and a list of tokens and returns a single parse of the tokens as specified by the grammar, or `None` if there are no parses. You should only need to add a few lines of code to your CKY recognizer to achieve this, to implement the necessary back-pointers. The function should return an NLTK tree, which can be constructed using `Tree.fromstring`.\n",
        "\n",
        "A tree string will be like this example:\n",
        "\n",
        "```\n",
        "\"(S (A B) (C (D E) (F G)))\"\n",
        "```\n",
        "\n",
        "which corresponds to the following tree (drawn using tree.pretty_print()):\n",
        "```\n",
        "     S\n",
        "  ___|___\n",
        " |       C\n",
        " |    ___|___\n",
        " A   D       F\n",
        " |   |       |\n",
        " B   E       G\n",
        "```\n",
        "\n",
        "> **Hint:** You may want to extract from a `Nonterminal` its corresponding string. The `Nonterminal.__str__` method or f-string `f'{Nonterminal}'` accomplishes this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7ce8e8",
      "metadata": {
        "id": "7a7ce8e8"
      },
      "outputs": [],
      "source": [
        "## TODO -- Implement a CKY parser\n",
        "def cky_parse(grammar, tokens):\n",
        "  \"\"\"Returns an NLTK parse tree of the list of tokens `tokens` as\n",
        "  specified by the `grammar`. If there are multiple valid parses,\n",
        "  return any one of them.\n",
        "\n",
        "  Returns None if `tokens` is not parsable.\n",
        "  Implements the CKY algorithm, and therefore assumes `grammar` is in\n",
        "  Chomsky normal form.\n",
        "  \"\"\"\n",
        "  assert(grammar.is_chomsky_normal_form())\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "616a8a4a",
      "metadata": {
        "id": "616a8a4a"
      },
      "source": [
        "You can test your code on the test sentences provided above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa736151",
      "metadata": {
        "id": "fa736151"
      },
      "outputs": [],
      "source": [
        "for sentence in test_sentences:\n",
        "  tree = cky_parse(atis_grammar_cnf, tokenize(sentence))\n",
        "  if not tree:\n",
        "    print(f\"failed to parse: {sentence}\")\n",
        "  else:\n",
        "    xform.un_cnf(tree, atis_grammar_wunaries)\n",
        "    tree.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0abb984",
      "metadata": {
        "id": "e0abb984"
      },
      "source": [
        "You can also compare against the built-in NLTK parser that we constructed above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bd5285",
      "metadata": {
        "id": "33bd5285"
      },
      "outputs": [],
      "source": [
        "for sentence in test_sentences:\n",
        "  refparses = [p for p in atis_parser_expanded.parse(tokenize(sentence))]\n",
        "  predparse = cky_parse(atis_grammar_cnf, tokenize(sentence))\n",
        "  if predparse:\n",
        "    xform.un_cnf(predparse, atis_grammar_wunaries)\n",
        "\n",
        "  print('Reference parses:')\n",
        "  for reftree in refparses:\n",
        "    print(reftree)\n",
        "\n",
        "  print('\\nPredicted parse:')\n",
        "  print(predparse)\n",
        "\n",
        "  if (not predparse and len(refparses) == 0) or predparse in refparses:\n",
        "    print(\"\\nSUCCESS!\")\n",
        "  else:\n",
        "    print(\"\\nOops. No match.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754ec97a",
      "metadata": {
        "id": "754ec97a"
      },
      "source": [
        "Again, we test the coverage as a way of verifying that your parser works consistently with the recognizer and the NLTK parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "865a10a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "865a10a8",
        "outputId": "8d0d2808-35f9-438d-8ded-114f0d6f5cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [00:00<00:00, 581512.46it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "coverage(lambda sent: cky_parse(atis_grammar_cnf, sent),\n",
        "         training_corpus, n=4000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707fd40a",
      "metadata": {
        "id": "707fd40a"
      },
      "source": [
        "# Probabilistic CFG parsing via the CKY algorithm\n",
        "\n",
        "In practice, we want to work with grammars that cover nearly all the language we expect to come across for a given application. This leads to an explosion of rules and a large number of possible parses for any one sentence. To remove ambiguity between the different parses, it's desirable to move to probabilistic context-free grammars (PCFG). In this part of the assignment, you will construct a PCFG from training data, parse using a probabilistic version of CKY, and evaluate the quality of the resulting parses against gold trees.\n",
        "\n",
        "## Part 4: PCFG construction\n",
        "\n",
        "Compared to CFGs, PCFGs need to assign probabilities to grammar rules. For this goal, you'll write a function `pcfg_from_trees` that takes a list of strings describing a corpus of trees and returns an NLTK PCFG trained on that set of trees.\n",
        "\n",
        "> We expect you to implement `pcfg_from_trees` directly. You should **not** use the [`induce_pcfg`](https://www.nltk.org/api/nltk.grammar.html#nltk.grammar.induce_pcfg) function in implementing your solution.\n",
        "\n",
        "We want the PCFG to be in CNF format because the probabilistic version of CKY that you'll implement next also requires the grammar to be in CNF. However, the gold trees are not in CNF form, so in this case you will need to convert the gold *trees* to CNF before building the PCFG from them. To accomplish this, you should use the `treetransforms` package from `nltk`, which includes functions for converting to and from CNF. In particular, you'll want to make use of `treetransforms.collapse_unary` followed by `treetransforms.chomsky_normal_form` to convert a tree to its binarized version. You can then get the counts for all of the productions used in the trees, and then normalize them to probabilities so that the probabilities of all rules with the same left-hand side sum to 1.\n",
        "\n",
        "We'll use the `pcfg_from_trees` function that you define later for parsing.\n",
        "\n",
        "> To convert an `nltk.Tree` object `t` to CNF, you can use the below code. Note that it's different from the `xform` functions we used before as we are converting _trees_, not _grammars_.\n",
        ">\n",
        ">    ```\n",
        ">    treetransforms.collapse_unary(t, collapsePOS=True)\n",
        ">    treetransforms.chomsky_normal_form(t) # After this the tree will be in CNF\n",
        ">    ```\n",
        "\n",
        "> To construct a PCFG with a given start state and set of productions, see [`nltk.grammar.PCFG`](https://www.nltk.org/api/nltk.grammar.html#nltk.grammar.PCFG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db02aa2",
      "metadata": {
        "id": "0db02aa2"
      },
      "outputs": [],
      "source": [
        "#TODO - Define a function to convert a set of trees to a PCFG in Chomsky normal form.\n",
        "#You are not allowed to use any library functions except\n",
        "#`treetransforms.collapse_unary` and `treetransforms.chomsky_normal_form`,\n",
        "#write the logic by yourself.\n",
        "def pcfg_from_trees(trees, start=Nonterminal('TOP')):\n",
        "  \"\"\"Returns an NLTK PCFG in CNF with rules and counts extracted from a set of trees.\n",
        "\n",
        "  The `trees` argument is a list of strings in the form interpretable by\n",
        "  `Tree.fromstring`. The trees are converted to CNF using NLTK's\n",
        "  `treetransforms.collapse_unary` and `treetransforms.chomsky_normal_form`.\n",
        "\n",
        "  The `start` argument is the start nonterminal symbol of the returned\n",
        "  grammar.\"\"\"\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354929a0",
      "metadata": {
        "id": "354929a0"
      },
      "source": [
        "We can now train a PCFG on the *train* split `train.trees` that we downloaded in the setup at the start of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34def3e",
      "metadata": {
        "id": "a34def3e"
      },
      "outputs": [],
      "source": [
        "with open('data/train.trees') as file:\n",
        "  ## Convert the probabilistic productions to an NLTK probabilistic CFG.\n",
        "  pgrammar = pcfg_from_trees(file.readlines())\n",
        "\n",
        "## Verify that the grammar is in CNF\n",
        "assert(pgrammar.is_chomsky_normal_form())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa4c092b",
      "metadata": {
        "id": "fa4c092b"
      },
      "source": [
        "## Part 5: Probabilistic CKY parsing\n",
        "\n",
        "Finally, we are ready to implement probabilistic CKY parsing under PCFGs. Adapt the CKY parser from Part 3 to return the most likely parse and its **log probability** (base 2) given a PCFG. Note that to avoid underflows we want to work in the log space.\n",
        "> **Hint:** `production.logprob()` will return the log probability of a production rule `production`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bff00b",
      "metadata": {
        "id": "f3bff00b"
      },
      "outputs": [],
      "source": [
        "## TODO – Implement a CKY parser under PCFGs\n",
        "def cky_parse_probabilistic(grammar, tokens):\n",
        "  \"\"\"Returns the NLTK parse tree of `tokens` with the highest probability\n",
        "  as specified by the PCFG `grammar` and its log probability as a tuple.\n",
        "\n",
        "  Returns (None, -float('inf')) if `tokens` is not parsable.\n",
        "  Implements the CKY algorithm, and therefore assumes `grammar` is in\n",
        "  Chomsky normal form.\n",
        "  \"\"\"\n",
        "  assert(grammar.is_chomsky_normal_form())\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f907cc5",
      "metadata": {
        "id": "9f907cc5"
      },
      "source": [
        "As an aid in debugging, you may want to start by testing your implementation of probabilistic CKY on a much smaller grammar than the one you trained from the ATIS corpus. Here's a little grammar that you can play with.\n",
        "\n",
        "> **Hint:** By \"play with\", we mean that you can change the gramamr to try out the behavior of your parser on different test grammars, including ambiguous cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7a7397",
      "metadata": {
        "id": "db7a7397"
      },
      "outputs": [],
      "source": [
        "grammar = PCFG.fromstring(\"\"\"\n",
        "  S -> NP VP [1.0]\n",
        "  VP -> V NP [1.0]\n",
        "  PP -> P NP [1.0]\n",
        "  NP -> 'sam' [.3]\n",
        "  NP -> 'ham' [.7]\n",
        "  V -> 'likes' [1.0]\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d360fc90",
      "metadata": {
        "id": "d360fc90"
      },
      "outputs": [],
      "source": [
        "tree, logprob = cky_parse_probabilistic(grammar, tokenize('sam likes ham'))\n",
        "tree.pretty_print()\n",
        "print(f\"logprob: {logprob:4.3g} | probability: {2**logprob:4.3g}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c50128",
      "metadata": {
        "id": "92c50128"
      },
      "outputs": [],
      "source": [
        "# We don't use our tokenizer because the gold trees do not lowercase tokens\n",
        "sent = \"Flights from Cleveland to Kansas City .\".split()\n",
        "tree, logprob = cky_parse_probabilistic(pgrammar, sent)\n",
        "tree.un_chomsky_normal_form()\n",
        "tree.pretty_print()\n",
        "print(f\"logprob: {logprob:4.3g} | probability: {2**logprob:4.3g}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30e8d2b",
      "metadata": {
        "id": "f30e8d2b"
      },
      "source": [
        "## Evaluation of the grammar\n",
        "\n",
        "There are a number of ways to evaluate parsing algorithms. In this project segment, you will use the [\"industry-standard\" `evalb` implementation](https://nlp.cs.nyu.edu/evalb/) for computing constituent precision, recall, and F1 scores. We downloaded `evalb` during setup.\n",
        "\n",
        "We read in the test data..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9832c19c",
      "metadata": {
        "id": "9832c19c"
      },
      "outputs": [],
      "source": [
        "with open('data/test.trees') as file:\n",
        "  test_trees = [Tree.fromstring(line.strip()) for line in file.readlines()]\n",
        "\n",
        "test_sents = [tree.leaves() for tree in test_trees]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d79a5d",
      "metadata": {
        "id": "f9d79a5d"
      },
      "source": [
        "...and parse the test sentences using your probabilistic CKY implementation, writing the output trees to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0715fe3",
      "metadata": {
        "id": "f0715fe3"
      },
      "outputs": [],
      "source": [
        "trees_out = []\n",
        "for sent in tqdm(test_sents):\n",
        "  tree, prob = cky_parse_probabilistic(pgrammar, sent)\n",
        "  if tree is not None:\n",
        "    tree.un_chomsky_normal_form()\n",
        "    trees_out.append(tree.pformat(margin=9999999999))\n",
        "  else:\n",
        "    trees_out.append('()')\n",
        "\n",
        "with open('data/outp.trees', 'w') as file:\n",
        "  for line in trees_out:\n",
        "    file.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff39db0",
      "metadata": {
        "id": "2ff39db0"
      },
      "source": [
        "Now we can compare the predicted trees to the ground truth trees, using `evalb`. You should expect to achieve F1 of about 0.83."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ad9a92",
      "metadata": {
        "id": "c3ad9a92"
      },
      "outputs": [],
      "source": [
        "shell(\"python scripts/evalb.py data/outp.trees data/test.trees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8663e54a",
      "metadata": {
        "id": "8663e54a"
      },
      "source": [
        "## Debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on might include the following:\n",
        "\n",
        "* Was the project segment clear or unclear? Which portions?\n",
        "* Were the readings appropriate background for the project segment?\n",
        "* Are there additions or changes you think would make the project segment better?\n",
        "    ```\n",
        "    BEGIN QUESTION\n",
        "    name: open_response_debrief\n",
        "    manual: true\n",
        "    ```\n",
        "\n",
        "but you should comment on whatever aspects you found especially positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861ff95f",
      "metadata": {
        "id": "861ff95f"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71f9d56",
      "metadata": {
        "id": "d71f9d56"
      },
      "source": [
        "# Instructions for submission of the project segment\n",
        "\n",
        "This project segment should be submitted to Gradescope at <https://rebrand.ly/project3-submit-code> and <https://rebrand.ly/project3-submit-pdf>, which will be made available some time before the due date.\n",
        "\n",
        "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <https://rebrand.ly/project3-submit-code>. Make sure that you are also submitting your `data/grammar` file as part of your solution code as well.\n",
        "\n",
        "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <https://rebrand.ly/project3-submit-pdf>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da7a639",
      "metadata": {
        "id": "2da7a639"
      },
      "source": [
        "# End of project segment 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Project Segment 3: Parsing – The CKY Algorithm"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}